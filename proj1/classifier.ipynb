{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Reading comprehension\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Tutorial: https://lightning.ai/docs/pytorch/stable/starter/introduction.html#\n",
    "\n",
    "W&B Link: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Dependencies\n",
    "Install all necessary dependencies\n",
    "- PyTorch: `torch lightning`\n",
    "- Hugging Face: `huggingface_hub datasets`\n",
    "- Weights & Biases: `wandb`\n",
    "- nltk: `nltk`\n",
    "\n",
    "Optional\n",
    "- Lint and Formatting: `ruff`\n",
    "\n",
    "Dependencies are pinned to the version the code was created with. \n",
    "\n",
    "## Notebook setup\n",
    "Log into Hugging Face and Weights & Biases.\n",
    "\n",
    "## Tools used\n",
    "- Visual Studio Code\n",
    "- GitHub Copilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch huggingface_hub datasets wandb ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Predefined requirements:\n",
    "- Train / Validation / Test split\n",
    "- Existing word embedding model: word2vec, GloVe, fastText\n",
    "\n",
    "Download the BoolQ dataset with `datasets` and split it in the predefined way.\n",
    "\n",
    "Data treatment steps:\n",
    "- Tokenize\n",
    "- Lower case\n",
    "- Stop word removal\n",
    "- lemmatization \n",
    "- truncating of passage (enforce maximum length, ~99% not truncated)\n",
    "- Embedding with fasttext\n",
    "\n",
    "Used features: `question`, `answer`, `passage` (all of them)\n",
    "\n",
    "Input format: `question` and `passage` vectors\n",
    "Label format: `answer` 1 or 0\n",
    "\n",
    "Batching: None, dataset is small enough (6.5k rows in train)\n",
    "\n",
    "Correctness tests:\n",
    "Check texts before embedding if they still make sense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train = load_dataset(\"google/boolq\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"google/boolq\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Predefined requirements:\n",
    "- Classifier\n",
    "    - 2 Layers\n",
    "    - ReLu\n",
    "\n",
    "network architecture\n",
    "- input layer\n",
    "    - dim: max len of input x 2\n",
    "- 2 hidden layers\n",
    "    - 0.5 * max x 2\n",
    "    - ReLu activation\n",
    "- output layer\n",
    "    - output of class (1 true, 0 false)\n",
    "        - dim: 1x1\n",
    "        - not probability output because the question should be answered with yes or no, not 60% yes\n",
    "    - sigmoid activation\n",
    "- normalization: done in preprocessing\n",
    "- regularization: done by optimizer\n",
    "\n",
    "Loss function either:\n",
    "- HingeEmbeddingLoss: measures whether two inputs are similar or not\n",
    "- Binary Cross-Entropy: separate to classes\n",
    "Optimizer either:\n",
    "- Adam: good default choice\n",
    "- AdamW: supposed improvements to Adam\n",
    "\n",
    "Experiments:\n",
    "- Different Loss and Optimizer combiniations\n",
    "- Size of input & hidden layers\n",
    "- Epochs, Learning rate \n",
    "\n",
    "Checkpoints: every few epochs \n",
    "early stops: if loss does not improve\n",
    "\n",
    "correctness test:\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Initialize Weights & Biases project for the project\n",
    "\n",
    "Use k fold cross validation to avoid overfitting\n",
    "\n",
    "1. Define experiement with different hyperparameters\n",
    "2. Train model with train dataset split\n",
    "3. Check model performance with validation dataset split\n",
    "4. Log training run to Weights & Biases\n",
    "5. Repeat\n",
    "\n",
    "After all experiments have run select best runs hyperparameters for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Metrics:\n",
    "- F1 Score\n",
    "- AOC\n",
    "Averaging: ?\n",
    "Error analysis:\n",
    "- Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "Compare results of final model to expectations.\n",
    "Run evaluation of final model with test dataset split.\n",
    "But only at the very end, otherwise data leakage can happen.\n",
    "\n",
    "Expectation: 70% accuracy with test dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
