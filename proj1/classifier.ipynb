{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Reading comprehension\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Tutorial: https://lightning.ai/docs/pytorch/stable/starter/introduction.html#\n",
    "\n",
    "W&B Link: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Dependencies\n",
    "Install all necessary dependencies\n",
    "- PyTorch: `torch lightning`\n",
    "- Hugging Face: `huggingface_hub datasets`\n",
    "- Weights & Biases: `wandb`\n",
    "- nltk: `nltk`\n",
    "- optuna\n",
    "https://docs.ray.io/en/latest/tune/examples/tune-pytorch-lightning.html\n",
    "https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_lightning_simple.py\n",
    "\n",
    "Optional\n",
    "- Lint and Formatting: `ruff`\n",
    "\n",
    "Dependencies are pinned to the version the code was created with. \n",
    "\n",
    "## Notebook setup\n",
    "Log into Hugging Face and Weights & Biases.\n",
    "\n",
    "## Tools used\n",
    "- Visual Studio Code\n",
    "- GitHub Copilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/codespace/.local/lib/python3.12/site-packages (2.4.1+cpu)\n",
      "Requirement already satisfied: lightning in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.4.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.25.1)\n",
      "Requirement already satisfied: datasets in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: wandb in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.18.2)\n",
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ruff in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.6.8)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /home/codespace/.local/lib/python3.12/site-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from lightning) (0.11.7)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from lightning) (24.1)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from lightning) (1.4.2)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from lightning) (4.66.5)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/python/3.12.1/lib/python3.12/site-packages (from lightning) (2.4.0)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/codespace/.local/lib/python3.12/site-packages (from wandb) (4.3.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from wandb) (5.28.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from wandb) (2.14.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/python/3.12.1/lib/python3.12/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Collecting pybind11>=2.2 (from fasttext)\n",
      "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/codespace/.local/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.3-cp312-cp312-linux_x86_64.whl size=5206042 sha256=48d5807696dcfa98e3778a78c989edf5a2e83ccc50cfcd438757be84f7243e63\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/20/27/95/a7baf1b435f1cbde017cabdf1e9688526d2b0e929255a359c6\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "\u001b[33m  WARNING: The script pybind11-config is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed fasttext-0.9.3 pybind11-2.13.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch lightning huggingface_hub datasets wandb nltk fasttext ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch import optim, nn, utils\n",
    "import lightning as L\n",
    "import nltk\n",
    "import torch\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Predefined requirements:\n",
    "- Train / Validation / Test split\n",
    "- Existing word embedding model: word2vec, GloVe, fastText\n",
    "\n",
    "Download the BoolQ dataset with `datasets` and split it in the predefined way.\n",
    "\n",
    "Data treatment steps:\n",
    "- Tokenize\n",
    "- Lower case\n",
    "- Stop word removal\n",
    "- lemmatization \n",
    "- truncating of passage (enforce maximum length, ~99% not truncated)\n",
    "- Embedding with fasttext\n",
    "\n",
    "Used features: `question`, `answer`, `passage` (all of them)\n",
    "\n",
    "Input format: `question` and `passage` vectors\n",
    "Label format: `answer` 1 or 0\n",
    "\n",
    "Batching: None, dataset is small enough (6.5k rows in train)\n",
    "\n",
    "Correctness tests:\n",
    "Check texts before embedding if they still make sense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8427 1000 3270\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\")\n",
    "train_raw = load_dataset(\"google/boolq\", split=\"train[:-1000]\")\n",
    "valid_raw = load_dataset(\"google/boolq\", split=\"train[-1000:]\")\n",
    "test_raw = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "\n",
    "print(len(train_raw), len(valid_raw), len(test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = fasttext.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words = stop_words | {'\\\"', '\\'', '\\'\\'', '`', '``', '\\'s'}\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "embeddings = None\n",
    "\n",
    "\n",
    "text_values = [\"question\", \"passage\"]\n",
    "def preprocess_dataset(dataset):\n",
    "    for row in dataset:\n",
    "        for key in text_values:\n",
    "            embedded_value = [embedding_model[lemmatizer.lemmatize(word)] for word in nltk.word_tokenize(row[key].lower()) if word not in stop_words]\n",
    "            row[key] = embedded_value\n",
    "\n",
    "    return dataset\n",
    "\n",
    "#train = preprocess_dataset(train_raw)\n",
    "valid = preprocess_dataset(valid_raw)\n",
    "#test = preprocess_dataset(test_raw)\n",
    "valid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Predefined requirements:\n",
    "- Classifier\n",
    "    - 2 Layers\n",
    "    - ReLu\n",
    "\n",
    "network architecture\n",
    "- input layer\n",
    "    - dim: max len of input x 2\n",
    "- 2 hidden layers\n",
    "    - 0.5 * max x 2\n",
    "    - ReLu activation\n",
    "- output layer\n",
    "    - output of class (1 true, 0 false)\n",
    "        - dim: 1x1\n",
    "        - not probability output because the question should be answered with yes or no, not 60% yes\n",
    "    - sigmoid activation\n",
    "- normalization: done in preprocessing\n",
    "- regularization: done by optimizer\n",
    "\n",
    "Loss function either:\n",
    "- HingeEmbeddingLoss: measures whether two inputs are similar or not\n",
    "- Binary Cross-Entropy: separate to classes\n",
    "Optimizer either:\n",
    "- Adam: good default choice\n",
    "- AdamW: supposed improvements to Adam\n",
    "\n",
    "Experiments:\n",
    "- Different Loss and Optimizer combiniations\n",
    "- Size of input & hidden layers\n",
    "- Epochs, Learning rate \n",
    "\n",
    "Checkpoints: every few epochs \n",
    "early stops: if loss does not improve\n",
    "\n",
    "correctness test:\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QClassifier(L.LightningModule):\n",
    "    def __init__(self, loss, optimizer, in_size=64, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_size, 2), nn.ReLU(), nn.Linear(in_size / 2, 2)\n",
    "        )\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(in_size / 2, 2), nn.ReLU(), nn.Linear(in_size / 2, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_size / 2, 2), nn.Sigmoid(), nn.Linear(1, 1)\n",
    "        )\n",
    "\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = self.loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer(self.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "loss = nn.functional.mse_loss\n",
    "optimizer = optim.Adam\n",
    "classifier = QClassifier(loss, optimizer)\n",
    "logger = WandbLogger(log_model=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Initialize Weights & Biases project for the project\n",
    "\n",
    "Use k fold cross validation to avoid overfitting\n",
    "\n",
    "1. Define experiement with different hyperparameters\n",
    "2. Train model with train dataset split\n",
    "3. Check model performance with validation dataset split\n",
    "4. Log training run to Weights & Biases\n",
    "5. Repeat\n",
    "\n",
    "After all experiments have run select best runs hyperparameters for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(limit_train_batches=100, max_epochs=1)\n",
    "train_loader = utils.data.DataLoader(train)\n",
    "trainer.fit(model=classifier, train_dataloaders=train_loader, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Metrics:\n",
    "- F1 Score\n",
    "- AOC\n",
    "Averaging: ?\n",
    "Error analysis:\n",
    "- Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "Compare results of final model to expectations.\n",
    "Run evaluation of final model with test dataset split.\n",
    "But only at the very end, otherwise data leakage can happen.\n",
    "\n",
    "Expectation: 70% accuracy with test dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
