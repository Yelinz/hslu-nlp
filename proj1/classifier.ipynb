{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Reading comprehension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "\n",
    "W&B Link: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Dependencies\n",
    "Install all necessary dependencies\n",
    "- PyTorch: `torch`\n",
    "- Hugging Face: `huggingface_hub datasets`\n",
    "- Weights & Biases: `wandb`\n",
    "\n",
    "Optional\n",
    "- Lint and Formatting: `ruff`\n",
    "\n",
    "Dependencies are pinned to the version the code was created with. \n",
    "\n",
    "## Notebook setup\n",
    "Log into Hugging Face and Weights & Biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch huggingface_hub datasets wandb ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Predefined requirements:\n",
    "- Train / Validation / Test split\n",
    "\n",
    "Download the BoolQ dataset and split it in the predefined way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google/boolq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Predefined requirements:\n",
    "- Existing word embedding: word2vec, GloVe, fastText\n",
    "- Classifier\n",
    "    - 2 Layers\n",
    "    - ReLu\n",
    "\n",
    "Define PyTorch model with needed layers\n",
    "\n",
    "Transform dataset texts with word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Initialize Weights & Biases project for the project\n",
    "\n",
    "1. Define experiement with different hyperparameters\n",
    "2. Train model with train dataset split\n",
    "3. Check model performance with validation dataset split\n",
    "4. Log training run to Weights & Biases\n",
    "5. Repeat\n",
    "\n",
    "After all experiments have run select best runs hyperparameters for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Run evaluation of final model with test dataset split.\n",
    "Only at the very end, otherwise data leakage can happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "Compare results of final model to expectations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
