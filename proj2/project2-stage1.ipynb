{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: RNN Boolq\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "\n",
    "W&B Link: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Notebook setup\n",
    "- Install all dependencies. `pip install`\n",
    "- Import all necessary libraries.\n",
    "- Log into Hugging Face and Weights & Biases.\n",
    "- Download GloVe Wikipedia embeddings from HuggingFace and unzip them in the `data` folder.\n",
    "- Load embeddings into `embeddings` dictionary variable.\n",
    "\n",
    "## Dependencies\n",
    "Install all necessary dependencies\n",
    "- PyTorch: `torch lightning`\n",
    "- Hugging Face: `huggingface_hub datasets`\n",
    "- Weights & Biases: `wandb`\n",
    "- nltk: `nltk`\n",
    "- numpy: `numpy`\n",
    "\n",
    "Optional\n",
    "- Lint and Formatting: `ruff`\n",
    "\n",
    "## Tools used\n",
    "- GPUHub JupyterLab\n",
    "- Pytorch Lightning documentation\n",
    "- Project 1 as skeleton for texts which do not need to change\n",
    "- No AI tools used, as 90% of their generated code is either broken or I do not understand and therefore useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Predefined requirements:\n",
    "- Train / Validation / Test split\n",
    "- Existing word embedding model: word2vec, GloVe, fastText\n",
    "- Download the BoolQ dataset with `datasets` and split it in the predefined way.\n",
    "\n",
    "Data treatment steps:\n",
    "- Lower case `text.lower()`\n",
    "    - Reason being the used GloVe pretrained embeddings on Wikipedia are uncased\n",
    "- Tokenize with nltk\n",
    "    - Use `word_tokenize` as we are interested in every word for word embedding with GloVe\n",
    "- Remove punctuation and non ascii characters (phoenetics etc.)\n",
    "    - Punctuation and non ascii characters are not relevant for answering questions\n",
    "    - Stop words are not removed as they are relevant for answering the question\n",
    "    - Remove ascii by encoding and decoding with `ascii`. `text.encode('ascii', 'ignore').decode('ascii')`\n",
    "    - Remove punctuation by checking against `string.punctuation` \n",
    "- Word embedding with GloVe (pretrained on Wikipedia)\n",
    "    - Perfered to word2vec because GloVe works with co-occurence and answering questions is about context\n",
    "    - Prefered to fastText because through previous processing no subword embeddings are needed\n",
    "    - Skip the word if it is not in the vocabulary of `embeddings`\n",
    "- Truncating by averaging of passage\n",
    "    - This is needed as in the dataset there are a few very long outliers, which would bloat the input to the model\n",
    "    - Enforce a maximum length, where ~99% of remaining passages not truncated `np.percentile(passages_lengths, 99)`\n",
    "    - Take the average of what should be truncated and add it to the end of the passage vector\n",
    "- Padding with 0 for question and passage for minimum length `np.pad`\n",
    "    - This is needed for the concatenation of question and passage, as they need to have the same length\n",
    "    - Pad all questions to maximum length of all questions\n",
    "    - Pad all passages to maximum length of all passages determined previously\n",
    "- Concatenate question and passage as the input for the model\n",
    "    - `np.concatenate` is used to have a single input for the model, which is not in an extra dimension as when using `np.stack`\n",
    "    - Add a seperator of 8 zeros between question and passage \n",
    "        - This is needed to be able to differentiate between question and passage\n",
    "- Remove `question` and `passage` columns from the dataset\n",
    "    - They are not needed anymore as they are now part of the input\n",
    "    - `dataset.remove_columns([\"question\", \"passage\"])`\n",
    "\n",
    "Used features:\n",
    "- `question` and `passage` as word vectors\n",
    "- `answer` as label\n",
    "\n",
    "Input format: concatenated `question` and `passage` word vectors (max length of question + seperator + max length of passage x embedding size)\n",
    "\n",
    "Label format:\n",
    "- convert `answer` boolean to 1 or 0\n",
    "- Model output is probability of 1\n",
    "- `dataset.cast_column(\"answer\", Value(\"int32\"))`\n",
    "\n",
    "Batch size: 64 for faster training\n",
    "\n",
    "## Correctness tests\n",
    "- Check processed passages and questions before embedding if they still make sense \n",
    "- Check embedding lengths\n",
    "- Check how many words are not in the vocabulary and maybe adjust which pretrained GloVe emebeddings are used based on that\n",
    "\n",
    "## Implementation\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Predefined requirements:\n",
    "- RNN\n",
    "    - LSTM or GRU\n",
    "- Classifier\n",
    "    - 2 Layers\n",
    "    - ReLu\n",
    "\n",
    "## Network Architecture\n",
    "- Input layer\n",
    "    - `torch.nn.GRU`\n",
    "    - Input Dimension: query + question word vector dimension  \n",
    "    - Output Dimension: hidden layer dimension\n",
    "    - Activation: `torch.nn.ReLu`\n",
    "- Output layer\n",
    "    - `torch.nn.GRU`\n",
    "    - Input Dimension: hidden layer dimension\n",
    "    - Output Dimension: 1\n",
    "        - Output is probability of class (1 = 100% true, 0 = 0% true)\n",
    "    - Activation: `torch.sigmoid`\n",
    "- Normalization: [GloVe word vectors are already normalized](https://github.com/JungeAlexander/GloVe/blob/master/eval/python/evaluate.py#L29-L33) \n",
    "- Regularization: done by optimizer\n",
    "\n",
    "Using GRU because it is simpler than LSTM and has similar performance, while being faster to train because it has one gate less.\n",
    "\n",
    "### Loss function\n",
    "Binary Cross-Entropy: \n",
    "- Best choice for binary classification problems\n",
    "- `torch.nn.BCELoss`\n",
    "\n",
    "### Optimizer\n",
    "AdamW:\n",
    "- Better with less hyperparamater tuning than SGD and the default Adam\n",
    "- `torch.optim.AdamW`\n",
    "\n",
    "## Experiments\n",
    "- Hidden layers dimension (128, 256, 512)\n",
    "    - To check if more complex models are needed\n",
    "- Dropout (0, 1e-1, 2e-1)\n",
    "    - To check how much regularization is needed (avoid under/overfitting)\n",
    "- Learning rate (1e-3, 1e-4, 1e-5)\n",
    "    - To check which learning rate is optimal\n",
    "    - No learning rate scheduler is needed as AdamW handles adjusting learning rates dynamically on its own with the passed learning rate being the maximum\n",
    "- Weight Decay (0, 1e-1, 1e-2)\n",
    "    - To check how much regularization is needed (avoid under/overfitting)\n",
    "\n",
    "### Checkpoints\n",
    "Best epochs based on validation balanced accuracy:\n",
    "- uploaded to wandb for later use\n",
    "- `ModelCheckpoint(save_top_k=10, monitor=\"val_balanced_acc\", mode=\"max\")`\n",
    "\n",
    "### Early stop\n",
    "Compare to previous epochs validation balanced accuracy\n",
    "- wandb sweeps use the Hyperband algorithm\n",
    "- Max epochs 500\n",
    "- Check every 10 epochs\n",
    "\n",
    "## Correctness test\n",
    "Test run of training, validation, test and prediction with 1 input\n",
    "\n",
    "## Implementation\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "- Use wandb sweeps for hyperparameter tuning.\n",
    "    - Grid search will be used, as the hyperparameter choices are discrete and the search space is not too large (3x3x3x3 = 81 experiments)\n",
    "    - Manually doing many experiments is tedious therefore use wandb sweeps\n",
    "    - Best integration into wandb instead of other libraries such as optuna, ray\n",
    "    - `wandb.sweep`\n",
    "\n",
    "- Log training and validation metrics to wandb after every epoch\n",
    "    - Log at end of epoch by using `training_epoch_end` and `validation_epoch_end`\n",
    "    - Balanced accuracy\n",
    "        - Accuracy is not a good metric for imbalanced datasets, as it can be misleading\n",
    "        - `torchmetrics.functional.classification.accuracy(preds, target, task='multiclass', num_classes=2, average='macro')`\n",
    "    - Loss\n",
    "        - Loss should decrease over time\n",
    "    - Precision Recall curve\n",
    "        - Show the tradeoff between precision and recall\n",
    "        - `torchmetrics.functional.classification.precision_recall_curve(pred, target, task='multiclass', num_classes=2)`\n",
    "    - F1\n",
    "        - F1 is a better performance measure than accuracy in imbalanced datasets \n",
    "        - `torchmetrics.functional.classification.f1_score(preds, target, task='multiclass', num_classes=2, average='macro')`\n",
    "- Run validation after every epoch\n",
    "    - To check how the model is doing on unseen data\n",
    "\n",
    "After all experiments have run select best runs based on the balanced accuracy as the final model to be evaluated.\n",
    "\n",
    "Balanced accuracy is the decision metric as it also includes the negative predictions, unlike F1. We are also interested in the negatives because they also have to be predicted correctly for question answering.\n",
    "\n",
    "## Implementation\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Most metric implementation will reuse the code from the training phase, as they are the same.\n",
    "\n",
    "Additionally the accuracy and confusion matrix will also be examined. Both will only be implemented for the evaluation step. \n",
    "- Accuracy is to be able to compare the model to the previous project\n",
    "    - As well as to check how it compares to the dataset imbalance\n",
    "    - Additionally because accuracy is easier to understand as a metric than balanced accuracy\n",
    "    - `torchmetrics.functional.classification.accuracy(preds, target, task='multiclass', num_classes=2, average='micro')`\n",
    "- The confusion matrix is to be able to see where the model tends to make mistakes.\n",
    "    - If it only predicts one class or of it mixes in predictions of the other class\n",
    "    - `torchmetrics.functional.confusion_matrix(preds, target, num_classes=2)`\n",
    "\n",
    "Metrics used for evaluation:\n",
    "- Accuracy\n",
    "- Balanced Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- Confusion matrix\n",
    "\n",
    "\n",
    "Load the best model from wandb artifact registry.\n",
    "\n",
    "Run evaluation of final model with test and validation dataset.\n",
    "\n",
    "There will be no changing of parameters after the final model has been evaluated. As that would be train-test leakage.\n",
    "\n",
    "## Implementation\n",
    "TODO\n",
    "\n",
    "## Result\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "Expectation:\n",
    "- 55% balanced accuracy with test dataset. As this would be better than just randomly guessing if the answer to the question is true or false.\n",
    "- 65% accuracy with test dataset. As this would be better than the test label imbalance of 62.2% true labels.\n",
    "\n",
    "\n",
    "## Result and learnings\n",
    "TODO\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
