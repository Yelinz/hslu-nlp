{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: RNN BoolQ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Classification of BoolQ with RNNs.\n",
    "\n",
    "The BoolQ dataset is preprocessed to be used with GloVe word embeddings. Then `question` and `passage` are concatenated to be the input to the classification model. Which results in an input of 304x300.\n",
    "\n",
    "A classification model with 2 GRU layers is created and connected with 2 linear layers for outputting the probability if the passage answers the question.\n",
    "The linear layers are connected through ReLu and Sigmoid is used to output the probabilities.\n",
    "\n",
    "Experiments were made with various hyperparameters configuations to determine which works best. Those were hidden layer size, learning rate, dropout and weight decay.\n",
    "\n",
    "The final model was with hidden layer size 256, learning rate 0.001, dropout 0 and weight decay 0\n",
    "\n",
    "With following performances:\n",
    "- Balanced accuracy: 54%\n",
    "- F1: 44%\n",
    "- Loss: 7.1\n",
    "- Precision: 69%\n",
    "- Recall: 65%\n",
    "- Accuracy: 64%\n",
    "\n",
    "\n",
    "W&B Link: https://wandb.ai/yelin-zhang-hslu/nlp-project-2/workspace?nw=5v0ojjkq1bf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Install dependencies, import used libraries and download used embeddings.\n",
    "\n",
    "## Tools used\n",
    "- GPUHub JupyterLab\n",
    "- Pytorch Lightning documentation\n",
    "- No AI tools used, as they do not help with reading API documentation and GitHub issues \n",
    "\n",
    "## Changes to stage 1\n",
    "- Split up documentation and add description of chapter.\n",
    "- HuggingFace login is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "Python 3.11.9\n",
    "\n",
    "Install all necessary dependencies\n",
    "- PyTorch: `torch lightning`\n",
    "- Hugging Face: `huggingface_hub datasets`\n",
    "- Weights & Biases: `wandb`\n",
    "- nltk: `nltk`\n",
    "- numpy: `numpy`\n",
    "\n",
    "Optional\n",
    "- Lint and Formatting: `ruff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.3.1 in /opt/conda/lib/python3.11/site-packages (2.3.1+cu121)\n",
      "Requirement already satisfied: lightning==2.4.0 in /opt/conda/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: huggingface_hub==0.25.2 in /opt/conda/lib/python3.11/site-packages (0.25.2)\n",
      "Requirement already satisfied: datasets==3.0.1 in /opt/conda/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: wandb==0.18.3 in /opt/conda/lib/python3.11/site-packages (0.18.3)\n",
      "Requirement already satisfied: nltk==3.9.1 in /opt/conda/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: ruff==0.6.9 in /opt/conda/lib/python3.11/site-packages (0.6.9)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.3.1) (2.3.1)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.11/site-packages (from lightning==2.4.0) (6.0.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from lightning==2.4.0) (0.11.8)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.11/site-packages (from lightning==2.4.0) (24.1)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from lightning==2.4.0) (1.4.3)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.11/site-packages (from lightning==2.4.0) (4.66.4)\n",
      "Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.11/site-packages (from lightning==2.4.0) (2.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub==0.25.2) (2.32.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets==3.0.1) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets==3.0.1) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets==3.0.1) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets==3.0.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets==3.0.1) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets==3.0.1) (3.10.10)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.3) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.3) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.3) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.3) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.3) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.3) (6.0.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.3) (2.17.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.3) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.3) (71.0.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk==3.9.1) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk==3.9.1) (2024.9.11)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.5.82)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb==0.18.3) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==3.0.1) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==3.0.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==3.0.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==3.0.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==3.0.1) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==3.0.1) (1.15.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb==0.18.3) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub==0.25.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub==0.25.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub==0.25.2) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub==0.25.2) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.3.1) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==3.0.1) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==3.0.1) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==3.0.1) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch==2.3.1) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.18.3) (5.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==3.0.1) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.3.1 lightning==2.4.0 huggingface_hub==0.25.2 datasets==3.0.1 wandb==0.18.3 nltk==3.9.1 numpy==1.26.4 ruff==0.6.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup\n",
    "- Import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import Value\n",
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import torchmetrics\n",
    "from torch import optim, nn, utils\n",
    "import lightning as L\n",
    "import nltk\n",
    "import torch\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Log into Hugging Face and Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=\"project2-stage2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myelin-zhang\u001b[0m (\u001b[33myelin-zhang-hslu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "%env \"WANDB_NOTEBOOK_NAME\" \"project2-stage2\"\n",
    "wandb.login()\n",
    "WANDB_PROJECT = \"nlp-project-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download GloVe Wikipedia embeddings from HuggingFace and unzip them in the `data` folder.\n",
    "\n",
    "> Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_PRETRAINED = \"glove.6B.zip\"\n",
    "model_path = hf_hub_download(repo_id=\"stanfordnlp/glove\", filename=GLOVE_PRETRAINED)\n",
    "shutil.unpack_archive(model_path, \"./glove_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load embeddings into `embeddings` dictionary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "with open(\"./glove_embeddings/glove.6B.300d.txt\") as glove:\n",
    "    for word_vec in glove:\n",
    "        values = word_vec.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Predefined requirements:\n",
    "- Train / Validation / Test split\n",
    "- Existing word embedding model: word2vec, GloVe, fastText\n",
    "- Download the BoolQ dataset with `datasets` and split it in the predefined way.\n",
    "\n",
    "Data treatment steps:\n",
    "- Lower case `text.lower()`\n",
    "    - Reason being the used GloVe pretrained embeddings on Wikipedia are uncased\n",
    "- Tokenize with nltk\n",
    "    - Use `word_tokenize` as we are interested in every word for word embedding with GloVe\n",
    "- Remove punctuation and non ascii characters (phoenetics etc.)\n",
    "    - Punctuation and non ascii characters are not relevant for answering questions\n",
    "    - Stop words are not removed as they are relevant for answering the question\n",
    "    - Remove ascii by encoding and decoding with `ascii`. `text.encode('ascii', 'ignore').decode('ascii')`\n",
    "    - Remove punctuation by checking against `string.punctuation` \n",
    "- Word embedding with GloVe (pretrained on Wikipedia)\n",
    "    - Perfered to word2vec because GloVe works with co-occurence and answering questions is about context\n",
    "    - Prefered to fastText because through previous processing no subword embeddings are needed\n",
    "    - Skip the word if it is not in the vocabulary of `embeddings`\n",
    "- Truncating by averaging of passage\n",
    "    - This is needed as in the dataset there are a few very long outliers, which would bloat the input to the model\n",
    "    - Enforce a maximum length, where ~99% of remaining passages not truncated `np.percentile(passages_lengths, 99)`\n",
    "    - Take the average of what should be truncated and add it to the end of the passage vector\n",
    "- Padding with 0s for question and passage for minimum length `np.pad`\n",
    "    - This is needed for the concatenation of question and passage, as they need to have the same length\n",
    "    - Pad all questions to maximum length of all questions\n",
    "    - Pad all passages to maximum length of all passages determined previously\n",
    "- Concatenate question and passage as the input for the model\n",
    "    - `np.concatenate` is used to have a single input for the model, which is not in an extra dimension as when using `np.stack`\n",
    "    - Add a seperator of 8 vectors with only zeros between question and passage \n",
    "        - This is needed to be able to differentiate between question and passage\n",
    "- Remove `question` and `passage` columns from the dataset\n",
    "    - They are not needed anymore as they are now part of the input\n",
    "    - `dataset.remove_columns([\"question\", \"passage\"])`\n",
    "\n",
    "Used features:\n",
    "- `question` and `passage` as word vectors\n",
    "- `answer` as label\n",
    "\n",
    "Input format: concatenated `question` and `passage` word vectors (max length of question + seperator + max length of passage x embedding size)\n",
    "\n",
    "Label format:\n",
    "- convert `answer` boolean to 1 or 0\n",
    "- Model output is probability of 1\n",
    "- `dataset.cast_column(\"answer\", Value(\"int32\"))`\n",
    "\n",
    "Batch size: 64 for faster training\n",
    "\n",
    "## Correctness tests\n",
    "- Check processed passages and questions before embedding if they still make sense \n",
    "- Check embedding lengths\n",
    "- Check how many words are not in the vocabulary and maybe adjust which pretrained GloVe emebeddings are used based on that\n",
    "\n",
    "## Changes to stage 1\n",
    "- Clarification of seperator between question and passage. Not 0 character, but multiple vectors with only 0 values as seperator.\n",
    "- After concatenation the data is being reshaped, to reduce the input dimension.\n",
    "\n",
    "## Implementation\n",
    "The preprocessing is split into two main steps.\n",
    "1. Creation of the word vectors\n",
    "2. Converting into desired input format\n",
    "\n",
    "These are being run seperately to be able to do the padding and truncation.\n",
    "\n",
    "HuggingFace datasets cache computations, which caused an issue as global variables are set by the function which will be missing when not executed. This caused a sweep to finish with the wrong dimensions. Therefore causing me to have to execute the same swepp again with the correct dimensions.\n",
    "\n",
    "Preprocessing computation has to be reasonably fast, as the GPUHub seems to lose the Jupyter kernel after a few hours requiring me to rerun it. \n",
    "\n",
    "The result of the correctness checks is:\n",
    "- The answer to the question can still be pieced together just before creating the `passage` embeddings, therefore the processing was done succesfully.\n",
    "- Downloaded Glove Word embeddings are 300 long and the result is as expected.\n",
    "- About 8000 words are not in the vocabulary, but from looking at the missed words they do not seem very important.\n",
    "- The dimensional reduction of the model input `query` results in the expected size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and split dataset in predefined way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8427 1000 3270\n"
     ]
    }
   ],
   "source": [
    "# Predefined dataset loading code\n",
    "train_raw = load_dataset(\"google/boolq\", split=\"train[:-1000]\")\n",
    "valid_raw = load_dataset(\"google/boolq\", split=\"train[-1000:]\")\n",
    "test_raw = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "\n",
    "print(len(train_raw), len(valid_raw), len(test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lower case `text.lower()`\n",
    "    - Reason being the used GloVe pretrained embeddings on Wikipedia are uncased\n",
    "- Tokenize with nltk\n",
    "    - Use `word_tokenize` as we are interested in every word for word embedding with GloVe\n",
    "- Remove punctuation and non ascii characters (phoenetics etc.)\n",
    "    - Punctuation and non ascii characters are not relevant for answering questions\n",
    "    - Stop words are not removed as they are relevant for answering the question\n",
    "    - Remove ascii by encoding and decoding with `ascii`. `text.encode('ascii', 'ignore').decode('ascii')`\n",
    "    - Remove punctuation by checking against `string.punctuation` \n",
    "- Word embedding with GloVe (pretrained on Wikipedia)\n",
    "    - Perfered to word2vec because GloVe works with co-occurence and answering questions is about context\n",
    "    - Prefered to fastText because through previous processing no subword embeddings are needed\n",
    "    - Skip the word if it is not in the vocabulary of `embeddings`\n",
    "\n",
    "- Check processed passages and questions before embedding if they still make sense \n",
    "\n",
    "Why not:\n",
    "- stemming/ lemmatization\n",
    "    - GloVe pretrained embeddings do not use either technique, to keep the preprocessing same I also do not apply stemming or lemmatization\n",
    "- removal of other words/ stopwords\n",
    "    - Stopwords are important to answering the question, as negations and other important words are counted as stopwords.\n",
    "    - Other words are automatically removed if they do not appear in the embedding vocabulary\n",
    "- format cleaning\n",
    "    - Removing non ascii can be counted as format cleaning, but otherwise the dataset is already cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "question:\n",
      "lowered and ascii: do iran and afghanistan speak the same language\n",
      "tokenized: ['do', 'iran', 'and', 'afghanistan', 'speak', 'the', 'same', 'language']\n",
      "filtered: ['do', 'iran', 'and', 'afghanistan', 'speak', 'the', 'same', 'language']\n",
      "\n",
      "passage:\n",
      "lowered and ascii: persian (/prn, -n/), also known by its endonym farsi ( frsi (fsi) ( listen)), is one of the western iranian languages within the indo-iranian branch of the indo-european language family. it is primarily spoken in iran, afghanistan (officially known as dari since 1958), and tajikistan (officially known as tajiki since the soviet era), and some other regions which historically were persianate societies and considered part of greater iran. it is written in the persian alphabet, a modified variant of the arabic script, which itself evolved from the aramaic alphabet.\n",
      "tokenized: ['persian', '(', '/prn', ',', '-n/', ')', ',', 'also', 'known', 'by', 'its', 'endonym', 'farsi', '(', 'frsi', '(', 'fsi', ')', '(', 'listen', ')', ')', ',', 'is', 'one', 'of', 'the', 'western', 'iranian', 'languages', 'within', 'the', 'indo-iranian', 'branch', 'of', 'the', 'indo-european', 'language', 'family', '.', 'it', 'is', 'primarily', 'spoken', 'in', 'iran', ',', 'afghanistan', '(', 'officially', 'known', 'as', 'dari', 'since', '1958', ')', ',', 'and', 'tajikistan', '(', 'officially', 'known', 'as', 'tajiki', 'since', 'the', 'soviet', 'era', ')', ',', 'and', 'some', 'other', 'regions', 'which', 'historically', 'were', 'persianate', 'societies', 'and', 'considered', 'part', 'of', 'greater', 'iran', '.', 'it', 'is', 'written', 'in', 'the', 'persian', 'alphabet', ',', 'a', 'modified', 'variant', 'of', 'the', 'arabic', 'script', ',', 'which', 'itself', 'evolved', 'from', 'the', 'aramaic', 'alphabet', '.']\n",
      "filtered: ['persian', '/prn', '-n/', 'also', 'known', 'by', 'its', 'endonym', 'farsi', 'frsi', 'fsi', 'listen', 'is', 'one', 'of', 'the', 'western', 'iranian', 'languages', 'within', 'the', 'indo-iranian', 'branch', 'of', 'the', 'indo-european', 'language', 'family', 'it', 'is', 'primarily', 'spoken', 'in', 'iran', 'afghanistan', 'officially', 'known', 'as', 'dari', 'since', '1958', 'and', 'tajikistan', 'officially', 'known', 'as', 'tajiki', 'since', 'the', 'soviet', 'era', 'and', 'some', 'other', 'regions', 'which', 'historically', 'were', 'persianate', 'societies', 'and', 'considered', 'part', 'of', 'greater', 'iran', 'it', 'is', 'written', 'in', 'the', 'persian', 'alphabet', 'a', 'modified', 'variant', 'of', 'the', 'arabic', 'script', 'which', 'itself', 'evolved', 'from', 'the', 'aramaic', 'alphabet']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209205667dc146cfb61153cf2dedb830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2cc9b3889a49379bec29fb2cfc68c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17f2e3f09ea4f76ab13ab235beb7f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get punctuations to be removed\n",
    "punctuation = set(list(string.punctuation))\n",
    "\n",
    "text_values = [\"question\", \"passage\"]\n",
    "passage_lens = []\n",
    "max_question_len = 0\n",
    "\n",
    "missed_words = set()\n",
    "embedding_vocabulary = set(embeddings.keys())\n",
    "\n",
    "\n",
    "def preprocess_dataset(row, debug=False):\n",
    "    global max_question_len\n",
    "    for key in text_values:\n",
    "        # lower case the text and remove non-ascii characters\n",
    "        lowered_value = row[key].lower().encode(\"ascii\", errors=\"ignore\").decode()\n",
    "        # tokenize the text\n",
    "        tokenized_value = [word for word in nltk.word_tokenize(lowered_value)]\n",
    "        # filter out punctuations\n",
    "        filtered_value = [word for word in tokenized_value if word not in punctuation]\n",
    "        # convert to word vectors\n",
    "        embedded_value = []\n",
    "        for word in filtered_value:\n",
    "            if word not in embedding_vocabulary:\n",
    "                missed_words.add(word)\n",
    "                continue\n",
    "            embedded_value.append(embeddings[word])\n",
    "        row[key] = embedded_value\n",
    "        if debug:\n",
    "            print(\n",
    "                f\"\\n{key}:\\nlowered and ascii: {lowered_value}\"\n",
    "                f\"\\ntokenized: {tokenized_value}\"\n",
    "                f\"\\nfiltered: {filtered_value}\"\n",
    "            )\n",
    "    # save passage lengths\n",
    "    passage_lens.append(len(row[\"passage\"]))\n",
    "    # save longest question length\n",
    "    q_len = len(row[\"question\"])\n",
    "    if q_len > max_question_len:\n",
    "        max_question_len = q_len\n",
    "    return row\n",
    "\n",
    "\n",
    "# can overwrite important global parameters if executed later\n",
    "preprocess_dataset(train_raw[0], True)\n",
    "\n",
    "train_vectorized = train_raw.map(preprocess_dataset, load_from_cache_file=False)\n",
    "valid_vectorized = valid_raw.map(preprocess_dataset, load_from_cache_file=False)\n",
    "test_vectorized = test_raw.map(preprocess_dataset, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check embedding lengths\n",
    "- Check how many words are not in the vocabulary and maybe adjust which pretrained GloVe emebeddings are used based on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "missed words 8623 ['decimetre', '/brbeds/', 'degree-level', 'refectories', 'j.s', 'dump-truck', 'posteriore', 'late-16th', 'time-gap', 'egnx', 'member-turned', 'wros', '23,876,155', 'driveclub', 'non-trace', 'trans-meridian', 'panther-platform', 'ethenyl', 'thyrsiflora', 'book-to-film', '19.', 'cruller', \"'an\", '4,842', 'fransmart', '1812.', 'francisco/daly', '150,682,490', 'press-ready', 'shokugeki', 'algee', 'robertson-dworet', 'stopping.', 'aw4', '738,432', 'shenell', 'pdf/x-1a', 'pgbt', '1.3333.', 'ho-gul', '109,673.', '244,106', '130,000,000', '/nti', 'harberts', 'sochna', 'penitus', 'i-476', 'rs-a', 'coffee-mate']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nmissed words\", len(missed_words), list(missed_words)[:50])\n",
    "\n",
    "assert len(train_vectorized[0][\"passage\"][0]) == 300, \"Word vecor should be 300 long\"\n",
    "assert len(missed_words) > 5, \"Too few missed words, preprocessing failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert answer boolean to 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_casted = train_vectorized.cast_column(\"answer\", Value(\"int32\"))\n",
    "valid_casted = valid_vectorized.cast_column(\"answer\", Value(\"int32\"))\n",
    "test_casted = test_vectorized.cast_column(\"answer\", Value(\"int32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Truncating by averaging of passage\n",
    "    - This is needed as in the dataset there are a few very long outliers, which would bloat the input to the model\n",
    "    - Enforce a maximum length, where ~99% of remaining passages not truncated `np.percentile(passages_lengths, 99)`\n",
    "    - Take the average of what should be truncated and add it to the end of the passage vector\n",
    "- Padding with 0 for question and passage for minimum length `np.pad`\n",
    "    - This is needed for the concatenation of question and passage, as they need to have the same length\n",
    "    - Pad all questions to maximum length of all questions\n",
    "    - Pad all passages to maximum length of all passages determined previously\n",
    "- Concatenate question and passage as the input for the model\n",
    "    - `np.concatenate` is used to have a single input for the model, which is not in an extra dimension as when using `np.stack`\n",
    "    - Add a seperator of 8 zeros between question and passage \n",
    "        - This is needed to be able to differentiate between question and passage\n",
    "     \n",
    "Why not:\n",
    "- averaging question and passage to a \"sentence vector\" and then concatenate\n",
    "    - The max length of questions is very short, therefore the gain would be minimal while losing more information.\n",
    "    - Passages are longer but I assume by not averaging to one vector more contextual information can be preserved. Such as word ordering. Which would result in slightly better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get 99th percentile of passage lengths, which will be used to truncate the data\n",
    "max_passage_len = int(np.percentile(passage_lens, 99))\n",
    "\n",
    "\n",
    "question_passage_padding = 8\n",
    "\n",
    "\n",
    "def truncate_pad_data(row):\n",
    "    # truncate by averaging too long passages to max_passage_len\n",
    "    if row[\"passage\"].shape[0] > max_passage_len:\n",
    "        to_average = row[\"passage\"][max_passage_len - 1 :]\n",
    "        row[\"passage\"][max_passage_len - 1] = np.mean(to_average, axis=0)\n",
    "        row[\"passage\"] = row[\"passage\"][:max_passage_len]\n",
    "    # pad too short passages to max_passage_len\n",
    "    elif row[\"passage\"].shape[0] < max_passage_len:\n",
    "        row[\"passage\"] = np.pad(\n",
    "            row[\"passage\"], ((0, max_passage_len - row[\"passage\"].shape[0]), (0, 0))\n",
    "        )\n",
    "\n",
    "    # pad too short questions to max_question_len plus question passage seperator\n",
    "    if row[\"question\"].shape[0] < max_question_len + question_passage_padding:\n",
    "        row[\"question\"] = np.pad(\n",
    "            row[\"question\"],\n",
    "            (\n",
    "                (\n",
    "                    0,\n",
    "                    max_question_len\n",
    "                    + question_passage_padding\n",
    "                    - row[\"question\"].shape[0],\n",
    "                ),\n",
    "                (0, 0),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # concatenate question and passage to a single input\n",
    "    row[\"query\"] = np.concatenate((row[\"question\"], row[\"passage\"]), axis=0).reshape(-1)\n",
    "    return row\n",
    "\n",
    "\n",
    "train_concatenated = train_casted.with_format(\"np\").map(truncate_pad_data)\n",
    "valid_concatenated = valid_casted.with_format(\"np\").map(truncate_pad_data)\n",
    "test_concatenated = test_casted.with_format(\"np\").map(truncate_pad_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check second preprocessing pass for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91200,)\n",
      "input size: (21+8+275)*300=91200\n"
     ]
    }
   ],
   "source": [
    "input_size = (max_question_len + question_passage_padding + max_passage_len) * 300\n",
    "print(train_concatenated[0][\"query\"].shape)\n",
    "print(f\"input size: ({max_question_len}+8+{max_passage_len})*300={input_size}\")\n",
    "assert train_concatenated[0][\"query\"].shape == (\n",
    "    input_size,\n",
    "), f\"Query shape should be {input_size}\"\n",
    "assert train_concatenated[1][\"query\"].shape == (\n",
    "    input_size,\n",
    "), f\"Query shape should be {input_size}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unnecessary columns as they are represented in `query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_concatenated.remove_columns([\"question\", \"passage\"])\n",
    "valid = valid_concatenated.remove_columns([\"question\", \"passage\"])\n",
    "test = test_concatenated.remove_columns([\"question\", \"passage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Predefined requirements:\n",
    "- RNN\n",
    "    - LSTM or GRU\n",
    "- Classifier\n",
    "    - 2 Layers\n",
    "    - ReLu\n",
    "\n",
    "## Network Architecture\n",
    "- Input layer\n",
    "    - `torch.nn.GRU`\n",
    "    - Input Dimension: query + question word vector dimension  \n",
    "    - Output Dimension: hidden layer dimension\n",
    "    - Activation: `torch.nn.ReLu`\n",
    "- Output layer\n",
    "    - `torch.nn.GRU`\n",
    "    - Input Dimension: hidden layer dimension\n",
    "    - Output Dimension: 1\n",
    "        - Output is probability of class (1 = 100% true, 0 = 0% true)\n",
    "    - Activation: `torch.sigmoid`\n",
    "- Normalization: [GloVe word vectors are already normalized](https://github.com/JungeAlexander/GloVe/blob/master/eval/python/evaluate.py#L29-L33) \n",
    "- Regularization: done by optimizer\n",
    "\n",
    "Using GRU because it is simpler than LSTM and has similar performance, while being faster to train because it has one gate less.\n",
    "\n",
    "### Loss function\n",
    "Binary Cross-Entropy: \n",
    "- Best choice for binary classification problems\n",
    "- `torch.nn.BCELoss`\n",
    "\n",
    "### Optimizer\n",
    "AdamW:\n",
    "- Better with less hyperparamater tuning than SGD and the default Adam\n",
    "- `torch.optim.AdamW`\n",
    "\n",
    "## Correctness test\n",
    "Test run of training, validation, test and prediction with 1 input\n",
    "\n",
    "## Changes to stage 1\n",
    "- Forgot to define the linear classification layers.\n",
    "- Split documentation between code cells\n",
    "- Max epochs 50, as training took longer than expected\n",
    "\n",
    "## Implementation\n",
    "The correctness test of the model runs without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GRUQClassifier(L.LightningModule):\n",
    "    def __init__(self, hidden_size=128, dropout=0, lr=1e-3, weight_decay=0):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.hidden = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "\n",
    "        self.layer_rnn = nn.GRU(\n",
    "            input_size, hidden_size, num_layers=2, dropout=self.dropout\n",
    "        )\n",
    "        self.layer_1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer_2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(\n",
    "            self.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.layer_rnn(x)\n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctness test of the model definition, by running the model with one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5025]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = L.Trainer(fast_dev_run=True, limit_test_batches=1)\n",
    "m = GRUQClassifier()\n",
    "m(torch.from_numpy(train[:1][\"query\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Checkpoints\n",
    "Best epochs based on best validation balanced accuracy:\n",
    "- uploaded to wandb for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = L.pytorch.callbacks.ModelCheckpoint(\n",
    "    save_top_k=10, monitor=\"val_balanced_accuracy\", mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "- Hidden layers dimension (128, 256, 512)\n",
    "    - To check if more complex models are needed\n",
    "- Dropout (0, 1e-1, 2e-1)\n",
    "    - To check how much regularization is needed (avoid under/overfitting)\n",
    "- Learning rate (1e-3, 1e-4, 1e-5)\n",
    "    - To check which learning rate is optimal\n",
    "    - No learning rate scheduler is needed as AdamW handles adjusting learning rates dynamically on its own with the passed learning rate being the maximum\n",
    "- Weight Decay (0, 1e-1, 1e-2)\n",
    "    - To check how much regularization is needed (avoid under/overfitting)\n",
    "\n",
    "\n",
    "### Early stop\n",
    "Compare to previous epochs validation balanced accuracy\n",
    "- wandb sweeps use the Hyperband algorithm\n",
    "- Max epochs 50\n",
    "- Check every 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"val_balanced_accuracy\"},\n",
    "    \"parameters\": {\n",
    "        \"hidden_size\": {\"values\": [128, 256, 512]},\n",
    "        \"dropout\": {\"values\": [0, 1e-1, 2e-1]},\n",
    "        \"lr\": {\"values\": [1e-3, 1e-4, 1e-5]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-1, 1e-2]},\n",
    "    },\n",
    "    \"early_terminate\": {\"type\": \"hyperband\", \"max_iter\": 500, \"s\": 50, \"eta\": 3},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "- Log training and validation metrics to wandb after every epoch\n",
    "    - Log at end of epoch by using `training_epoch_end` and `validation_epoch_end`\n",
    "    - Balanced accuracy\n",
    "        - Accuracy is not a good metric for imbalanced datasets, as it can be misleading\n",
    "        - `torchmetrics.functional.classification.accuracy(preds, target, task='multiclass', num_classes=2, average='macro')`\n",
    "    - Loss\n",
    "        - Loss should decrease over time\n",
    "    - Precision Recall curve\n",
    "        - Show the tradeoff between precision and recall\n",
    "        - `torchmetrics.functional.classification.precision_recall_curve(pred, target, task='multiclass', num_classes=2)`\n",
    "    - F1\n",
    "        - F1 is a better performance measure than accuracy in imbalanced datasets \n",
    "        - `torchmetrics.functional.classification.f1_score(preds, target, task='multiclass', num_classes=2, average='macro')`\n",
    "\n",
    "## Changes to stage 1\n",
    "There was no need to log in the hooks `epoch_end` as pytorch lightning automatically configures the logger to log on epoch when using it in the `step` functions.\n",
    "https://lightning.ai/docs/pytorch/stable/extensions/logging.html#automatic-logging\n",
    "\n",
    "## Implementation\n",
    "Precision/Recall curves were used as there is a class imbalance.\n",
    "\n",
    "F1 was chosen to be macro as to reflect both classes F1 in one score instead of only the majority class like in project 1.\n",
    "\n",
    "There might be problems when continuing using the same jupyter kernel after running the sweep. https://docs.wandb.ai/guides/sweeps/start-sweep-agents/#stop-wb-agent\n",
    "\n",
    "Restarting the kernel to execute lower cells helps.\n",
    "\n",
    "The correctness test of the train and validation definition runs without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(self, batch, batch_idx):\n",
    "    _, _, loss, acc, f1_score, prc = self._get_pred_metrics(batch)\n",
    "    self.log_dict(\n",
    "        {\n",
    "            \"train_loss\": loss,\n",
    "            \"train_balanced_accuracy\": acc,\n",
    "            \"train_f1\": f1_score,\n",
    "            \"train_precision\": prc[0].mean(),\n",
    "            \"train_recall\": prc[1].mean(),\n",
    "            \"train_threshold\": prc[2].mean(),\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def _get_pred_metrics(self, batch):\n",
    "    answers = batch[\"answer\"]\n",
    "    pred = self(batch[\"query\"])\n",
    "    pred = pred.view(-1)\n",
    "\n",
    "    loss = self.loss(pred, answers.float())\n",
    "    acc = torchmetrics.functional.classification.accuracy(\n",
    "        pred, answers, task=\"multiclass\", num_classes=2, average=\"macro\"\n",
    "    )\n",
    "    prc = torchmetrics.functional.classification.precision_recall_curve(\n",
    "        pred, answers, task=\"binary\"\n",
    "    )\n",
    "    f1_score = torchmetrics.functional.classification.f1_score(\n",
    "        pred, answers, task=\"multiclass\", num_classes=2, average=\"macro\"\n",
    "    )\n",
    "    return pred, answers, loss, acc, f1_score, prc\n",
    "\n",
    "\n",
    "GRUQClassifier.training_step = training_step\n",
    "GRUQClassifier._get_pred_metrics = _get_pred_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run validation after every epoch\n",
    "    - To check how the model is doing on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(self, batch, batch_idx):\n",
    "    _, _, loss, acc, f1_score, prc = self._get_pred_metrics(batch)\n",
    "    self.log_dict(\n",
    "        {\n",
    "            \"val_loss\": loss,\n",
    "            \"val_balanced_accuracy\": acc,\n",
    "            \"val_f1\": f1_score,\n",
    "            \"val_precision\": prc[0].mean(),\n",
    "            \"val_recall\": prc[1].mean(),\n",
    "            \"val_threshold\": prc[2].mean(),\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "GRUQClassifier.validation_step = validation_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the train and validation was defined correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A16') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/jovyan/NLP/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | loss      | BCELoss | 0      | train\n",
      "1 | layer_rnn | GRU     | 35.2 M | train\n",
      "2 | layer_1   | Linear  | 16.5 K | train\n",
      "3 | layer_2   | Linear  | 129    | train\n",
      "----------------------------------------------\n",
      "35.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "35.2 M    Total params\n",
      "140.746   Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c802c26e34a6460b88d846fc8e446f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model=m,\n",
    "    train_dataloaders=utils.data.DataLoader(train, batch_size=4),\n",
    "    val_dataloaders=utils.data.DataLoader(train, batch_size=4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use wandb sweeps for hyperparameter tuning.\n",
    "    - Grid search will be used, as the hyperparameter choices are discrete and the search space is not too large (3x3x3x3 = 81 experiments)\n",
    "    - Manually doing many experiments is tedious therefore use wandb sweeps\n",
    "    - Best integration into wandb instead of other libraries such as optuna, ray\n",
    "    - `wandb.sweep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = utils.data.DataLoader(\n",
    "    train, batch_size=64, num_workers=2, pin_memory=True\n",
    ")\n",
    "valid_loader = utils.data.DataLoader(\n",
    "    valid, batch_size=64, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "def sweep():\n",
    "    with wandb.init(project=WANDB_PROJECT) as run:\n",
    "        name = f\"gru/hidden_size:{wandb.config['hidden_size']}/dropout:{wandb.config['dropout']}/lr:{wandb.config['lr']}/weight_decay:{wandb.config['weight_decay']}\"\n",
    "        run.name = name\n",
    "        logger = WandbLogger(project=WANDB_PROJECT, log_model=\"all\", name=name)\n",
    "        classifier = GRUQClassifier(**wandb.config)\n",
    "        trainer = L.Trainer(\n",
    "            max_epochs=50,\n",
    "            logger=logger,\n",
    "            accelerator=\"gpu\",\n",
    "            devices=1,\n",
    "            callbacks=[checkpoint],\n",
    "        )\n",
    "        try:\n",
    "            trainer.fit(\n",
    "                model=classifier,\n",
    "                train_dataloaders=train_loader,\n",
    "                val_dataloaders=valid_loader,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            wandb.finish()\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=experiments, project=WANDB_PROJECT)\n",
    "\n",
    "wandb.agent(sweep_id, function=sweep)\n",
    "wandb.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all experiments have run select best runs based on the balanced accuracy as the final model to be evaluated.\n",
    "\n",
    "Balanced accuracy is the decision metric as it also includes the negative predictions, unlike F1. We are also interested in the negatives because they also have to be predicted correctly for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best run gru/hidden_size:256/dropout:0/lr:0.001/weight_decay:0 with 0.5568439364433289 validation balanced accuracy\n"
     ]
    }
   ],
   "source": [
    "# check wandb for sweep id, if the notebook lost the variable\n",
    "SWEEP_ID = \"glglzbew\"\n",
    "\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"yelin-zhang-hslu/{WANDB_PROJECT}/{SWEEP_ID}\")\n",
    "runs = sorted(\n",
    "    sweep.runs,\n",
    "    key=lambda run: run.summary.get(\"val_balanced_accuracy\", 0),\n",
    "    reverse=True,\n",
    ")\n",
    "val_acc = runs[0].summary.get(\"val_balanced_accuracy\", 0)\n",
    "print(f\"Best run {runs[0].name} with {val_acc} validation balanced accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Most metric implementation will reuse the code from the training phase, as they are the same.\n",
    "\n",
    "Additionally the accuracy and confusion matrix will also be examined. Both will only be implemented for the evaluation step. \n",
    "- Accuracy is to be able to compare the model to the previous project\n",
    "    - As well as to check how it compares to the dataset imbalance\n",
    "    - Additionally because accuracy is easier to understand as a metric than balanced accuracy\n",
    "    - `torchmetrics.functional.classification.accuracy(preds, target, task='multiclass', num_classes=2, average='micro')`\n",
    "- The confusion matrix is to be able to see where the model tends to make mistakes.\n",
    "    - If it only predicts one class or of it mixes in predictions of the other class\n",
    "    - `torchmetrics.functional.confusion_matrix(preds, target, num_classes=2)`\n",
    "\n",
    "Metrics used for evaluation:\n",
    "- Accuracy\n",
    "- Balanced Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- Confusion matrix\n",
    "\n",
    "There will be no changing of parameters after the final model has been evaluated. As that would be train-test leakage.\n",
    "\n",
    "## Implementation\n",
    "Implementation of the confusion matrix in the pytorch lightning hooks is difficult for wandb. As there seems to be no good way to incementally log the values.\n",
    "Therefore it it implemented with `predict` seperatly\n",
    "\n",
    "## Result\n",
    "By evaluating the top 10 hyperparamter configurations the experiments revealed:\n",
    "- Hidden size did not matter much, as all sizes were represented and the best performing was not the largest\n",
    "- Larger Learning rate had better performance\n",
    "- Models with lower dropout performed better\n",
    "- Models with lower weight decay also performed better\n",
    "\n",
    "In total the best model was with Hidden size 256, learning rate 0.001, dropout 0 and weight decay 0\n",
    "\n",
    "With following performances:\n",
    "- Balanced accuracy\n",
    "    - Train: 83%\n",
    "    - Validation: 55%\n",
    "    - Test: 54%\n",
    "- F1\n",
    "    - Train: 81%\n",
    "    - Validation: 45%\n",
    "    - Test: 44%\n",
    "- Loss\n",
    "    - Train: 0\n",
    "    - Validation: 6.5\n",
    "    - Test: 7.1\n",
    "- Precision\n",
    "    - Train: 80%\n",
    "    - Validation: 66%\n",
    "    - Test: 69%\n",
    "- Recall\n",
    "    - Train: 93%\n",
    "    - Validation: 64%\n",
    "    - Test: 65%\n",
    "- Accuracy test: 64%\n",
    "\n",
    "With following confusion matrix (Switch to light mode if it does not look right in dark mode):\n",
    "\n",
    "![Confusion matrix](https://images2.imgbox.com/b8/49/xUx4cV9k_o.png)\n",
    "\n",
    "The model managed to predict many of the not answerable question correctly and did not overfit to the majority class.\n",
    "\n",
    "Validation and test performance are very close which is good, as the model did not manage to overfit on the validation data. But a large gap is in test to validation performance, this seems to only happen to the best runs. The 2nd, 3rd highest validation accuracy also have close test to validation accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(self, batch, batch_idx):\n",
    "    preds, answers, loss, balanced_acc, f1_score, prc = self._get_pred_metrics(batch)\n",
    "    acc = torchmetrics.functional.classification.accuracy(\n",
    "        preds, answers, task=\"binary\", num_classes=2\n",
    "    )\n",
    "    self.log_dict(\n",
    "        {\n",
    "            \"test_loss\": loss,\n",
    "            \"test_accuracy\": acc,\n",
    "            \"test_balanced_accuracy\": balanced_acc,\n",
    "            \"test_f1\": f1_score,\n",
    "            \"test_precision\": prc[0].mean(),\n",
    "            \"test_recall\": prc[1].mean(),\n",
    "            \"test_threshold\": prc[2].mean(),\n",
    "        }\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "    return self(batch[\"query\"])\n",
    "\n",
    "\n",
    "GRUQClassifier.test_step = test_step\n",
    "GRUQClassifier.predict_step = predict_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the implementation for test and predict are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9bb1d630ea417181c9e0f8c745a250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy                 1.0\n",
      " test_balanced_accuracy             0.0\n",
      "         test_f1                    0.0\n",
      "        test_loss           0.5471592545509338\n",
      "     test_precision                 1.0\n",
      "       test_recall                  0.5\n",
      "     test_threshold          0.578847348690033\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ecdc91435d4902816b9cab498fba0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[0.5503],\n",
       "         [0.5826],\n",
       "         [0.5876],\n",
       "         [0.5949]])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=m, dataloaders=utils.data.DataLoader(train, batch_size=4))\n",
    "trainer.predict(model=m, dataloaders=utils.data.DataLoader(train, batch_size=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best model from wandb artifact registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-y5epoh0i:best, 809.12MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/NLP/wandb/run-20241020_084639-y5epoh0i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/yelin-zhang-hslu/nlp-project-2/runs/y5epoh0i' target=\"_blank\">gru/hidden_size:256/dropout:0/lr:0.001/weight_decay:0</a></strong> to <a href='https://wandb.ai/yelin-zhang-hslu/nlp-project-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/yelin-zhang-hslu/nlp-project-2/sweeps/glglzbew' target=\"_blank\">https://wandb.ai/yelin-zhang-hslu/nlp-project-2/sweeps/glglzbew</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yelin-zhang-hslu/nlp-project-2' target=\"_blank\">https://wandb.ai/yelin-zhang-hslu/nlp-project-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/yelin-zhang-hslu/nlp-project-2/sweeps/glglzbew' target=\"_blank\">https://wandb.ai/yelin-zhang-hslu/nlp-project-2/sweeps/glglzbew</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yelin-zhang-hslu/nlp-project-2/runs/y5epoh0i' target=\"_blank\">https://wandb.ai/yelin-zhang-hslu/nlp-project-2/runs/y5epoh0i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_reference = f\"yelin-zhang-hslu/{WANDB_PROJECT}/model-{runs[0].id}:best\"\n",
    "\n",
    "artifact = api.artifact(checkpoint_reference).download()\n",
    "wandb.init(project=WANDB_PROJECT, id=runs[0].id, resume=\"allow\")\n",
    "\n",
    "classifier_final = GRUQClassifier.load_from_checkpoint(Path(artifact) / \"model.ckpt\")\n",
    "eval_logger = WandbLogger(project=WANDB_PROJECT, log_model=\"all\")\n",
    "eval_trainer = L.Trainer(\n",
    "    logger=eval_logger,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement confustion matrix calculation and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(trainer, classifier, dataset, name):\n",
    "    dataloader = utils.data.DataLoader(\n",
    "        dataset, batch_size=64, num_workers=2, pin_memory=True\n",
    "    )\n",
    "    pred_batches = trainer.predict(classifier, dataloaders=dataloader)\n",
    "    preds = np.array([])\n",
    "    for batch in pred_batches:\n",
    "        preds = np.concatenate((preds, batch.squeeze().numpy()))\n",
    "    conf = wandb.plot.confusion_matrix(\n",
    "        y_true=dataset[\"answer\"],\n",
    "        preds=np.round(preds),\n",
    "        class_names=[\"Incorrect\", \"Correct\"],\n",
    "    )\n",
    "    wandb.log({\"valid_conf_matrix\": conf})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation of final model with test and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4f4479cb844f39983f436b98b35a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     Validate metric           DataLoader 0\n",
      "\n",
      "  val_balanced_accuracy     0.5576755404472351\n",
      "         val_f1             0.4544157087802887\n",
      "        val_loss             6.581948757171631\n",
      "      val_precision         0.6660197377204895\n",
      "       val_recall           0.6403718590736389\n",
      "      val_threshold         0.6956357359886169\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0546f649b9674c318062cc9bab517de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a727635bd444c679071654350b706ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_accuracy          0.639755368232727\n",
      " test_balanced_accuracy     0.5482339262962341\n",
      "         test_f1            0.4425245523452759\n",
      "        test_loss            7.132151126861572\n",
      "     test_precision         0.6943557262420654\n",
      "       test_recall          0.6556441783905029\n",
      "     test_threshold         0.6782917976379395\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caca8e89fa6d4324aaf616617adc2167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_trainer.validate(classifier_final, dataloaders=valid_loader)\n",
    "create_confusion_matrix(eval_trainer, classifier_final, valid, \"valid_conf_matrix\")\n",
    "\n",
    "test_loader = utils.data.DataLoader(test, batch_size=64, num_workers=2, pin_memory=True)\n",
    "eval_trainer.test(classifier_final, dataloaders=test_loader)\n",
    "create_confusion_matrix(eval_trainer, classifier_final, test, \"test_conf_matrix\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "Expectation:\n",
    "- 55% balanced accuracy with test dataset. As this would be better than just randomly guessing if the answer to the question is true or false.\n",
    "- 65% accuracy with test dataset. As this would be better than the test label imbalance of 62.2% true labels.\n",
    "## Results\n",
    "Overall the model performed just about as expected with test balanced accuracy being 54.82% and accuracy 63.97%. The model managed to learn more of the complexity if the question is not answered by the passage (minority class) than when only using linear layers. Using only 2 RNN layers and 2 Linear layer is not able to completly capture the complexity of the question answering task, as many of the better performing runs seem to be overfitting, as both loss and accuracy are rising in the validation step.\n",
    "\n",
    "The final performance is slightly worse than expected, but the amount of epochs used to train were also reduced from 500 to 50. In that context the performance is very satisfactory as less time was needed than expected to reach this level of performance.\n",
    "\n",
    "Precision and recall seem balanced as the confusion matrix reflects.\n",
    "\n",
    "Interpretation of the experiments are following: \n",
    "\n",
    "One key point is that the maximum epochs were set to 50, therefore many of the steps to avoid overfitting were contributing negativly towards the accuracy in this scenario.\n",
    "If the more epochs were used dropout and weight decay would become more important.\n",
    "\n",
    "The learning rate is also affected by the low epoch amount, as the smaller learning rate requires more time to reach the same accuracy as larger learning rate.\n",
    "\n",
    "The hidden size of the model did not seem to matter as the input to the model could not capute the complexity of the question and passage.\n",
    "\n",
    "## Learning\n",
    "Most of the decision turned out to be decent.\n",
    "\n",
    "Using GloVe instead of fasttext seemed to be better, as I did not need the subword abilities of fasttext.\n",
    "\n",
    "The formulation for the preprocessing can be improved as the question passage seperator was misinterpreted as literal 0 characters not 0 vectors to be used as seperator.\n",
    "\n",
    "I misread the project task and forgot define the linear layers in stage 1, which was a blunder but not very devastating.\n",
    "\n",
    "One point where I experienced problems was with the checkpoints which are created. Eventhough the amount of checkpoints were restricted in comparision to project 1. I managed to fill up the 100GB Weights & Biases storages with 2 full sweeps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
