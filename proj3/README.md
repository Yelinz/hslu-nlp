# Project 3

No truncation if not needed (BERT)
Padding by batch not total

https://www.kaggle.com/code/zhuyuqiang/transformer-encoder-for-classification-in-pytorch

https://debuggercafe.com/text-classification-using-transformer-encoder-in-pytorch/

https://stackoverflow.com/questions/58092004/how-to-do-sequence-classification-with-pytorch-nn-transformer

https://n8henrie.com/2021/08/writing-a-transformer-classifier-in-pytorch/

https://stackoverflow.com/questions/62399243/transformerencoder-with-a-padding-mask

https://www.nltk.org/api/nltk.tokenize.punkt.html

https://pytorch.org/torchtune/0.3/generated/torchtune.modules.RotaryPositionalEmbeddings.html

## Feedback stage 1:

## Feedback stage 2: 

## Feedback presentation: 
