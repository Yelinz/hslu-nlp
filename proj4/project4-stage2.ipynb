{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Pretrained Transformer BoolQ\n",
    "\n",
    "The documentation is split into small chunks following the suggestion in class and from feedback for previous projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Classification of BoolQ with Pretrained Transformers.\n",
    "\n",
    "\n",
    "W&B Link: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Preliminary steps for setting getting the project running.\n",
    "\n",
    "## Tools used\n",
    "- GPUHub JupyterLab\n",
    "- No AI tools used, as they do not help with reading API documentation and GitHub issues \n",
    "- Previous projects documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "The notebook was created with:\n",
    "Python \n",
    "\n",
    "Install all necessary dependencies\n",
    "- Pytorch: `torch`\n",
    "- Hugging Face: `huggingface_hub transformers datasets`\n",
    "- Weights & Biases: `wandb`\n",
    "- numpy: `numpy`\n",
    "- scikit-learn: `scikit-learn`\n",
    "- Lint and Formatting: `ruff`\n",
    "\n",
    "Versions of dependencies are pinned for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install torch==2.3.1 huggingface_hub==1.25.2 transformers datasets==3.0.1 wandb==0.18.3 numpy==1.26.4 scikit-learn==1.5.2 ruff==0.6.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup\n",
    "Import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Value\n",
    "from datasets import load_dataset\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log into Hugging Face and Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_PROJECT = \"nlp-project-4\"\n",
    "os.environ[\"WANDB_PROJECT\"]=WANDB_PROJECT\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"]=\"project4-stage2\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Predefined requirements:\n",
    "- Download the BoolQ dataset with `datasets` and split it in the predefined way.\n",
    "- Train / Validation / Test split\n",
    "\n",
    "Used features:\n",
    "- `question` and `passage` as input to the model\n",
    "- `answer` as label\n",
    "\n",
    "Input format:\n",
    "- concatenated `question` and `passage` strings\n",
    "- with a special seperator token from the model vocabulary in the middle to differentiate between them\n",
    "- question before passage because to be able to answer the question it first hast to be known\n",
    "\n",
    "Label format:\n",
    "- convert `answer` boolean to 1 or 0\n",
    "- Model output is probability of 1\n",
    "\n",
    "Batch size: 64 for faster training than with individual samples\n",
    "\n",
    "A lot of preprocessing steps are not needed, because the predefined tokenizer for the model does most of the work. The input format is the raw text without any changes.\n",
    "The tokenizer does not do any stemming, stopword removal, lower casing, format cleaning. For unknown words a special token `[UNK]` is used.\n",
    "\n",
    "## Correctness tests\n",
    "- Check processed passages and questions if they still make sense \n",
    "\n",
    "## Implementation\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and split dataset in predefined way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = load_dataset(\"google/boolq\", split=\"train[:-1000]\")\n",
    "valid_raw = load_dataset(\"google/boolq\", split=\"train[-1000:]\")\n",
    "test_raw = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "\n",
    "print(len(train_raw), len(valid_raw), len(test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concat `question` and `passage` into `query`\n",
    "    - Add special seperator token `[SEP]` between them to distinguish both texts from another\n",
    "- Tokenize sentence with `DeBERTAV2Tokenizer`\n",
    "    - It handles the tokenization of the text with `SentencePiece` and the conversion to word vectors as well.\n",
    "    - `SentencePiece` is a subword tokenizer, which learns how to split the text into subwords\n",
    "    - Padding to the maximum input length of each batch is done by the `DataCollatorWithPadding` later\n",
    "    - Truncation should not be needed, as the maximum input length is quite large (over the usual 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert answer boolean to 1 or 0, because the model output is a probability of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_casted = train_raw.cast_column(\"answer\", Value(\"float32\"))\n",
    "valid_casted = valid_raw.cast_column(\"answer\", Value(\"float32\"))\n",
    "test_casted = test_raw.cast_column(\"answer\", Value(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"deberta-v3-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def preprocess_dataset(row):\n",
    "    query = row[\"question\"] + \"[SEP]\" + row[\"passage\"]\n",
    "    tokenized = tokenizer(query)\n",
    "    tokenized[\"labels\"] = row[\"answer\"]\n",
    "    return tokenized\n",
    "\n",
    "train_concatenated = train_casted.map(preprocess_dataset)\n",
    "valid_concatenated = valid_casted.map(preprocess_dataset)\n",
    "test_concatenated = test_casted.map(preprocess_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unnecssary `question` and `passage` columns, as they are represented in `query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_concatenated.remove_columns([\"question\", \"passage\"])\n",
    "valid = valid_concatenated.remove_columns([\"question\", \"passage\"])\n",
    "test = test_concatenated.remove_columns([\"question\", \"passage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Predefined requirements:\n",
    "- pretrained Transformer encoder (from Hugging Face; it must not be finetuned on the BoolQ dataset yet)\n",
    "- Classifier\n",
    "    - 2 Layers\n",
    "    - ReLu\n",
    "\n",
    "## Network Architecture\n",
    "- Transformer encoder\n",
    "    - `DeBERTa-v3-large`\n",
    "    - Input Dimension: Batch size x 128100\n",
    "    - Output Dimension: Batch size x 768\n",
    "    - It was chosen as the latest version of the `DeBERTa` series, which have improvement over the original `BERT` model.\n",
    "        - With improvements in attention and pre training over `BERT` and `RoBRTa`.\n",
    "        - the `large` variant is used because I assume it will fit into memory and performs better than the base variant\n",
    "        - The `v2` tokenizer is compatible with the `v3` model\n",
    "- Classifier\n",
    "    - `nn.Linear`\n",
    "        - Input Dimension: 768 (Pooled output of `DeBERTa`)\n",
    "        - Output Dimension: 256\n",
    "        - Activation: `torch.nn.ReLu`\n",
    "        - Output shape is smaller than input, to promote the model to learn more abstract features for the classification\n",
    "    - `nn.Dropout`\n",
    "        - Dropout rate: 0.1\n",
    "        - Dropout is used to prevent overfitting, which happens easily with such a small dataset\n",
    "    - `nn.Linear`\n",
    "        - Input Dimension: 256\n",
    "        - Output Dimension: 1\n",
    "            - Output is probability of class (1 = 100% true, 0 = 0% true)\n",
    "        - Final Activation: `torch.nn.Sigmoid`\n",
    "\n",
    "- Normalization: Done in the `DeBERTa` model with their Masked Layer Normalization\n",
    "- Regularization: Optimizer `AdamW` applies L2 regularization to loss, no regularization layer is in `DeBERTa`\n",
    "\n",
    "### Loss function\n",
    "Default by transformers library: Binary Cross-Entropy with logit loss:\n",
    "- Not changed because it is the best choice for binary classification problems\n",
    "- and with logits can be better than only Binary cross entropy because it is supposedly more numerically stable\n",
    "\n",
    "### Optimizer\n",
    "Default by transformers library: `AdamW`\n",
    "- Not changed because it performs well and the original `DeBERTa` was also trained with a version of `AdamW`\n",
    "\n",
    "## Correctness test\n",
    "Test run of training, validation, test and prediction with 1 input\n",
    "Check transformer encoder output shapes\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, problem_type=\"multi_label_classification\", num_labels=1)\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(768, 526),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.1),\n",
    "    torch.nn.Linear(258, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctness test of the model definition, by running the model with one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints\n",
    "Save checkpoints at end of training with `transformers.integrations.WandbCallback` configuration and further configuration later in `TrainingArguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Various Learning rates (1 - 1e-10) in a uniform distribution\n",
    "- Extensive sweep to check which learning rate is optimal\n",
    "- No learning rate scheduler is needed as AdamW handles adjusting learning rates dynamically on its own with the passed learning rate being the maximum\n",
    "\n",
    "No other experiments are done, because there are not many other parameters which can be changed, as we are using a pre trained model. Additionally finding the optimal learning rate is the most important part of training the model.\n",
    "- The classifier hidden sizes are not changed as they are intended to be decreasing. For the model to pre compress the information before the binary classifcation.\n",
    "- No Weight decay because it might affect the pre trained model negativly\n",
    "- Learning rate warm up is not done, because max epochs are relativly low  \n",
    "\n",
    "\n",
    "### Early stop\n",
    "Early stopping is done by wandb sweeps.\n",
    "- Compare to previous epochs validation loss\n",
    "- wandb sweeps use the Hyperband algorithm\n",
    "- Max epochs 50\n",
    "- Check every 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"method\": \"bayesian\",\n",
    "    \"metric\": {\"goal\": \"minimize\", \"name\": \"val_loss\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-4, 1e-5]},\n",
    "    },\n",
    "    \"early_terminate\": {\"type\": \"hyperband\", \"max_iter\": 50, \"s\": 10, \"eta\": 3},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Training is done with the `Trainer` class from the `transformers` library.\n",
    "Configure training and evaluation with `TrainingArguments`. \n",
    "- set `seed` for reproducibility\n",
    "- `logging_strategy = 'epoch'` to log metrics after each epoch\n",
    "- `eval_strategy = 'epoch'` to evaluate after each epoch\n",
    "- `save_strategy = 'steps'` to save after ever 500 steps\n",
    "- `save_total_limit = 3` to save only the last 3 checkpoints\n",
    "- `label_names = ['answer']` to set the label name to the key we use\n",
    "- `report_to = 'wandb'` to log metrics to wandb\n",
    "- `run_name` to set the name of the run to an informative name\n",
    "- `dataloader_num_workers = 4` to speed up data loading\n",
    "- `per_device_train_batch_size = 64` to set the desired batch size for training\n",
    "- `per_device_eval_batch_size = 64` to set the desired batch size for evaluation\n",
    "- `num_train_epochs = 50` to set the desired max epochs\n",
    "\n",
    "Metrics for training and validation:\n",
    "- Accuracy, because we are interested in both correct true and false predictions\n",
    "- Loss, to see how confident the model is in its predictions\n",
    "- Metrics are logged every epoch. Because logging per step is very noisy and does not have a benefit.\n",
    "\n",
    "Loss is the main metric for all decisions, as it is the most important metric for the model. Accuracy should follow loss in a correct model. Therefore, it is not necessary to optimize for accuracy.\n",
    "\n",
    "As discussed in class no other metrics are needed for training and validation. As accuracy and loss are sufficient to evaluate which model is the best.\n",
    "\n",
    "Accuracy has to be implemented seperatly for training and evaluation, because `Trainer` from `transformers` only logs loss per default. \n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    seed=42,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    report_to=\"wandb\",\n",
    "    dataloader_num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use `wandb.sweep` for hyperparameter tuning.\n",
    "- Bayesian search will be used, because there is only one hyperparameter choices and it is continous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sweep():\n",
    "    with wandb.init(project=WANDB_PROJECT) as run:\n",
    "        name = f\"lr:{wandb.config['lr']}\"\n",
    "        run.name = name\n",
    "\n",
    "        train_args = TrainingArguments(\n",
    "            learning_rate=wandb.config['lr'],\n",
    "            seed=42,\n",
    "            logging_strategy=\"epoch\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"steps\",\n",
    "            save_total_limit=3,\n",
    "            report_to=\"wandb\",\n",
    "            dataloader_num_workers=4,\n",
    "        )\n",
    "        trainer = Trainer(model, train_args, train_dataset=train, eval_dataset=valid, tokenizer=tokenizer, compute_metrics=\"\")\n",
    "\n",
    "        name = f\"pos:{wandb.config['positional']}/heads:{wandb.config['heads']}/lr:{wandb.config['lr']}\"\n",
    "        try:\n",
    "            trainer.train()\n",
    "            trainer.evaluate()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            wandb.finish()\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=experiments, project=WANDB_PROJECT)\n",
    "\n",
    "wandb.agent(sweep_id, function=sweep, project=WANDB_PROJECT)\n",
    "wandb.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all experiments have run select best runs based on the smallest loss as the final model to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check wandb for sweep id\n",
    "SWEEP_ID = \"zmgqc6oi\"\n",
    "\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"yelin-zhang-hslu/{WANDB_PROJECT}/{SWEEP_ID}\")\n",
    "runs = sorted(\n",
    "    sweep.runs,\n",
    "    key=lambda run: run.summary.get(\"val_loss\", 99),\n",
    "    reverse=False,\n",
    ")\n",
    "val_loss = runs[0].summary.get(\"val_loss\", 99)\n",
    "print(f\"Best run {runs[0].name} with {val_loss} validation loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Metrics:\n",
    "- Accuracy\n",
    "    - to be able to compare the model to the previous projects\n",
    "    - As well as to check how it compares to the dataset imbalance\n",
    "    - `torchmetrics.functional.classification.accuracy(preds, target, task='binary')`\n",
    "- Confusion matrix\n",
    "    - To be able to see where the model tends to make mistakes.\n",
    "    - `torchmetrics.functional.confusion_matrix(preds, target, num_classes=2)`\n",
    "    - As discussed in class: use scikit-learn instead of wandb, as it is easier to interpret\n",
    "- Total false predictions\n",
    "    - To see how many false predictions the model made\n",
    "\n",
    "The averaging of the metrics is the default of `micro` which means the metrics are caculated without weighting of the classes.\n",
    "\n",
    "Evaluation will also be done with the `Trainer` class, just using the `evaluate` method and the test dataset.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "## Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(trainer, classifier, dataset):\n",
    "    dataloader = utils.data.DataLoader(\n",
    "        dataset, batch_size=64, num_workers=2, pin_memory=True, collate_fn=batch_collate\n",
    "    )\n",
    "    pred_batches = trainer.predict(classifier, dataloaders=dataloader)\n",
    "    preds = np.array([])\n",
    "    for batch in pred_batches:\n",
    "        preds = np.concatenate((preds, batch.squeeze().numpy()))\n",
    "    matrix = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true=dataset[\"answer\"],\n",
    "        y_pred=np.round(preds),\n",
    "    )\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the implementation for test and predict are correct by running it once and checking the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_reference = f\"yelin-zhang-hslu/{WANDB_PROJECT}/model-{runs[0].id}:best\"\n",
    "\n",
    "artifact = api.artifact(checkpoint_reference).download()\n",
    "wandb.init(project=WANDB_PROJECT, id=runs[0].id, resume=\"allow\")\n",
    "\n",
    "# classifier_final = TransformerClassifier.load_from_checkpoint(Path(artifact) / \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best model from wandb artifact registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement confusion matrix calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation of final model with test and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "Expectation:\n",
    "70% accuracy with test dataset. The expectation is that by using a good pretrained transformer encoder model (e.g. BERT family) that it has learned the semantics of words and sentences already. Therefore the classification of the questions should be easier.\n",
    "\n",
    "## Results\n",
    "\n",
    "## Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
