{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: LLM\n",
    "\n",
    "The documentation is split into small chunks following the suggestion in class and from feedback for previous projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "\n",
    "W&B Link: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Preliminary steps for setting getting the project running.\n",
    "\n",
    "## Tools used\n",
    "- GPUHub JupyterLab\n",
    "- No AI tools used, as they do not help with reading API documentation and GitHub issues \n",
    "- Previous projects documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "The notebook was created with:\n",
    "Python \n",
    "\n",
    "Install all necessary dependencies\n",
    "- Pytorch: `torch`\n",
    "- Hugging Face: `huggingface_hub transformers datasets peft`\n",
    "- Weights & Biases: `wandb`\n",
    "- numpy: `numpy`\n",
    "- scikit-learn: `scikit-learn`\n",
    "- Lint and Formatting: `ruff`\n",
    "\n",
    "Versions of dependencies are pinned for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup\n",
    "Import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log into Hugging Face and Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Predefined requirements:\n",
    "- Download the BoolQ dataset with `datasets` and split it in the predefined way.\n",
    "- Train / Validation / Test split\n",
    "\n",
    "Used features:\n",
    "- `question` and `passage` as input to the model\n",
    "- `answer` as label\n",
    "\n",
    "Input format:\n",
    "- concatenated `question` and `passage` strings\n",
    "- with a special seperator token from the model vocabulary in the middle to differentiate between them\n",
    "- question before passage because to be able to answer the question it first hast to be known\n",
    "\n",
    "Label format:\n",
    "- convert `answer` boolean to 1 or 0\n",
    "- Model output is probability of 1\n",
    "\n",
    "Batch size: 64 for faster training than with individual samples\n",
    "\n",
    "A lot of preprocessing steps are not needed, because the predefined tokenizer for the model does most of the work. The input format is the raw text without any changes.\n",
    "The tokenizer does not do any stemming, stopword removal, lower casing, format cleaning. For unknown words a special token `[UNK]` is used.\n",
    "\n",
    "## Correctness tests\n",
    "- Check processed passages and questions if they still make sense \n",
    "\n",
    "## Implementation\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and split dataset in predefined way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert answer boolean to 1 or 0, because the model output is a probability of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concat `question` and `passage` into `query`\n",
    "    - Add special seperator token `[SEP]` between them to distinguish both texts from another\n",
    "- Tokenize sentence with `LLAMA`\n",
    "    - It handles the tokenization of the text with `TikToken` and the conversion to word vectors as well.\n",
    "    - Padding to the maximum input length of each batch is done by the `DataCollatorWithPadding` later\n",
    "    - Truncation should not be needed, as the maximum input length is quite large ~8000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unnecssary `question` and `passage` columns, as they are represented in `query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Predefined requirements:\n",
    "- LLM (â‰¥ 1B parameters)\n",
    "- Use a quantized version as the base model\n",
    "\n",
    "## Network Architecture\n",
    "\n",
    "\n",
    "- Normalization: Done in the `DeBERTa` model with their Masked Layer Normalization\n",
    "- Regularization: Optimizer `AdamW` applies L2 regularization to loss, no regularization layer is in `DeBERTa`\n",
    "\n",
    "### Loss function\n",
    "Default by transformers library: Binary Cross-Entropy with logit loss:\n",
    "- Not changed because it is the best choice for binary classification problems\n",
    "- and with logits can be better than only Binary cross entropy because it is supposedly more numerically stable\n",
    "\n",
    "### Optimizer\n",
    "Default by transformers library: `AdamW`\n",
    "- Not changed because it performs well and the original `DeBERTa` was also trained with a version of `AdamW`\n",
    "\n",
    "## Correctness test\n",
    "Test run of training, validation, test and prediction with 1 input\n",
    "Check transformer encoder output shapes\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctness test of the model definition, by running the model with one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary evaluation with 5 diverse prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints\n",
    "Save checkpoints at end of training with `transformers.integrations.WandbCallback` configuration and further configuration later in `TrainingArguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Predefined requirements:\n",
    "- Then train it with parameter-efficient fine-tuning (I suggest LoRA, see e.g. the HF blog post or quicktour).\n",
    "\n",
    "\n",
    "Training is done with the `Trainer` class from the `transformers` library.\n",
    "Configure training and evaluation with `TrainingArguments`. \n",
    "- set `seed` for reproducibility\n",
    "- `logging_strategy = 'epoch'` to log metrics after each epoch\n",
    "- `eval_strategy = 'epoch'` to evaluate after each epoch\n",
    "- `save_strategy = 'steps'` to save after ever 500 steps\n",
    "- `save_total_limit = 3` to save only the last 3 checkpoints, otherwise limited wandb storage will overfill\n",
    "- `dataloader_num_workers = 2` to speed up data loading\n",
    "- `num_train_epochs = 20` use a low number of epochs, because every epoch will take a long time\n",
    "\n",
    "Metrics for training and validation:\n",
    "- Accuracy, because we are interested in both correct true and false predictions\n",
    "- Loss, to see how confident the model is in its predictions\n",
    "- Metrics are logged every epoch. Because logging per step is very noisy and does not have a benefit.\n",
    "\n",
    "Loss is the main metric for all decisions, as it is the most important metric for the model. Accuracy should follow loss in a correct model. Therefore, it is not necessary to optimize for accuracy.\n",
    "\n",
    "As discussed in class no other metrics are needed for training and validation. As accuracy and loss are sufficient to evaluate which model is the best.\n",
    "\n",
    "Accuracy has to be implemented seperatly for training and evaluation, because `Trainer` from `transformers` only logs loss per default.\n",
    "- Create a `TrainerCallback` for training accuracy\n",
    "- Define `compute_metrics` method for validation accuracy\n",
    "\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use `wandb.sweep` for hyperparameter tuning.\n",
    "- Bayesian search will be used, because there is only one hyperparameter choices and it is continous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all experiments have run select best runs based on the smallest loss as the final model to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Metrics:\n",
    "- Accuracy\n",
    "    - to be able to compare the model to the previous projects\n",
    "    - As well as to check how it compares to the dataset imbalance\n",
    "    - `torchmetrics.functional.classification.accuracy(preds, target, task='binary')`\n",
    "- Confusion matrix\n",
    "    - To be able to see where the model tends to make mistakes.\n",
    "    - `torchmetrics.functional.confusion_matrix(preds, target, num_classes=2)`\n",
    "    - As discussed in class: use scikit-learn instead of wandb, as it is easier to interpret\n",
    "- Total false predictions\n",
    "    - To see how many false predictions the model made\n",
    "\n",
    "The averaging of the metrics is the default of `micro` which means the metrics are caculated without weighting of the classes.\n",
    "\n",
    "Evaluation will also be done with the `Trainer` class, just using the `evaluate` method and the test dataset.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "## Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the implementation for test and predict are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best model from wandb artifact registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation of final model with test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "## Results\n",
    "\n",
    "## Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
