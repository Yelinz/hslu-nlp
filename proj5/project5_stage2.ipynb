{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monTUsFlzV09"
   },
   "source": [
    "# Project 5: LLM\n",
    "\n",
    "The documentation is split into small chunks following the suggestion in class and from feedback for previous projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV6b983_zV0-"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The task is to do a reading comprehension with a LLM.\n",
    "\n",
    "\n",
    "W&B Link: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsFh435AzV0_"
   },
   "source": [
    "# Setup\n",
    "Preliminary steps for setting getting the project running.\n",
    "\n",
    "## Tools used\n",
    "- GPUHub JupyterLab\n",
    "- No AI tools used, as they do not help with reading API documentation and GitHub issues\n",
    "- Previous projects documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqatKDxOzV0_"
   },
   "source": [
    "## Dependencies\n",
    "The notebook was created with:\n",
    "Python\n",
    "\n",
    "Install all necessary dependencies\n",
    "- Pytorch: `torch`\n",
    "- Hugging Face: `huggingface_hub transformers datasets peft trl`\n",
    "- Weights & Biases: `wandb`\n",
    "- numpy: `numpy`\n",
    "- scikit-learn: `scikit-learn`\n",
    "- Lint and Formatting: `ruff`\n",
    "\n",
    "Versions of dependencies are pinned for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loK2X2_7Ayq6",
    "outputId": "bb333d03-9700-4c9a-cb6b-ee8b7e1d2b47",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.11/site-packages (0.26.3)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: trl in /opt/conda/lib/python3.11/site-packages (0.12.1)\n",
      "Requirement already satisfied: wandb==0.18.7 in /opt/conda/lib/python3.11/site-packages (0.18.7)\n",
      "Requirement already satisfied: numpy==1.26.4 in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn==1.5.2 in /opt/conda/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: ruff==0.7.4 in /opt/conda/lib/python3.11/site-packages (0.7.4)\n",
      "Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.11/site-packages (4.46.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.7) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.7) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.7) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.7) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.7) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.7) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.7) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.7) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.7) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.7) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.7) (71.0.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /opt/conda/lib/python3.11/site-packages (from wandb==0.18.7) (4.12.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn==1.5.2) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn==1.5.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn==1.5.2) (3.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (1.1.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.11.9)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb==0.18.7) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb==0.18.7) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb==0.18.7) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb==0.18.7) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb==0.18.7) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb==0.18.7) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->trl) (2.18.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.18.7) (5.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch huggingface_hub transformers[torch] datasets peft trl wandb==0.18.7 numpy==1.26.4 scikit-learn==1.5.2 ruff==0.7.4\n",
    "#%pip install --upgrade transformers[torch] peft trl torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuQobpnBzV1A"
   },
   "source": [
    "## Notebook setup\n",
    "Import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eMsjJzqUzV1A"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "import sklearn\n",
    "from huggingface_hub import login as hf_login\n",
    "from trl import SFTConfig, SFTTrainer, AutoModelForCausalLMWithValueHead, DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iwyskIizV1A"
   },
   "source": [
    "Log into Hugging Face and Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "Q-08OpDqzV1B",
    "outputId": "748dcb0a-36ac-4b5a-d6ab-612934fa9780"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find ./project5-stage2.ipynb.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myelin-zhang\u001b[0m (\u001b[33myelin-zhang-hslu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WANDB_PROJECT = \"nlp-project-5\"\n",
    "os.environ[\"WANDB_PROJECT\"] = WANDB_PROJECT\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"./project5-stage2.ipynb\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vV8Zi7hJBjI3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472e4603a9e84c84a888230f5687a909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fU9jkZI8BVEh"
   },
   "outputs": [],
   "source": [
    "MODEL = \"meta-llama/Llama-3.2-1B\"\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09nzlgJ1zV1B"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "Predefined requirements:\n",
    "- Download the BoolQ dataset with `datasets` and split it in the predefined way.\n",
    "- Train / Validation / Test split\n",
    "\n",
    "Used features:\n",
    "- `question` and `passage` as input to the model\n",
    "- `answer` as label\n",
    "\n",
    "Input format:\n",
    "- `question` and `passage` as they are, later for training they will be combined in a template for the input prompt.\n",
    "\n",
    "Label format:\n",
    "- `answer` stays at is is for training later\n",
    "\n",
    "Batch size: 64 for faster training than with individual samples\n",
    "\n",
    "No further preprocessing is done, because the Llama can handle all text in the dataset as it is. Because it uses the `TikToken` Tokenzier under the hood.\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUbv5KWgzV1B"
   },
   "source": [
    "Download and split dataset in predefined way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "davtU3GazV1B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8427 1000 3270\n"
     ]
    }
   ],
   "source": [
    "train_raw = load_dataset(\"google/boolq\", split=\"train[:-1000]\")\n",
    "valid_raw = load_dataset(\"google/boolq\", split=\"train[-1000:]\")\n",
    "test_raw = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "\n",
    "print(len(train_raw), len(valid_raw), len(test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rmm9LrxkzV1D"
   },
   "source": [
    "# Model\n",
    "Predefined requirements:\n",
    "- LLM (≥ 1B parameters)\n",
    "- Use a quantized version as the base model\n",
    "\n",
    "Chosen model: Llama 3.2\n",
    "- 1.23B params\n",
    "- Quantized with SpinQuant and GPTQ\n",
    "\n",
    "Predictions:\n",
    "The model should generate either \"True\" or \"False\" to the question based on the given input. Based on that metrics can be calculated.\n",
    "If it predicts something else than the expected outputs, it will be counted seperatly as a failed prediction.\n",
    "\n",
    "Normalization: LLama has a RMSNorm layer.\n",
    "\n",
    "Regularization: Optimizer `AdamW` applies L2 regularization to loss. Unkown if there is regularization in Llama\n",
    "\n",
    "### Loss function\n",
    "Default by transformers library: Cross-Entropy:\n",
    "- Not changed because it is the best choice for classification problems\n",
    "\n",
    "### Optimizer\n",
    "Default by transformers library: `AdamW`\n",
    "- Not changed because it performs well, better than `Adam` as well.\n",
    "\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WJZWHj3ZzV1D",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Llama-3.2-1B', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    base_model_name_or_path=MODEL\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    MODEL,\n",
    "    #peft_config=lora_config,\n",
    "    max_length=8000,\n",
    "    device_map = 'cuda'\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(example, include_answer=True):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['question'])):\n",
    "        text = f\"### Question: {example['question'][i]}\\n ### Context: {example['passage'][i]}\\n ### Answer:\"\n",
    "        if include_answer:\n",
    "            text += str(example['answer'][i])\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1qFb82SzV1D"
   },
   "source": [
    "A correctness test of the model definition will be done, by running the model with one batch and checking the output.\n",
    "\n",
    "The generated text looks right and we can procede with implementing the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>do iran and afghanistan speak the same language\\nIran and Afghanistan are two countries that are very close to each other. They are both located in the Middle East and share many similarities. However']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenizer(train_raw[0][\"question\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "tokenizer.batch_decode(model.generate(tokenized[\"input_ids\"], max_time=1, pad_token_id=tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMue22W1zV1D"
   },
   "source": [
    "Predefined requirement: Preliminary evaluation with 5 diverse prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bpr715v7zV1D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0:\n",
      "<|begin_of_text|>### Question: do iran and afghanistan speak the same language\n",
      " ### Context: Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.\n",
      " ### Answer: Persian is a language spoken by 75 million people in Iran, Afghanistan, Tajikistan, and other countries. It is the official language of Iran and Tajikistan, and is also spoken by a significant minority in Afghanistan, Pakistan, and the United Arab Emirates. The language is also used in the Persian Gulf region, where it is spoken by a large number of people of Persian descent. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United Kingdom, where it is known as Persian. The language is also spoken by a significant minority in Australia, where it is known as Farsi. The language is also spoken by a significant minority in Canada, where it is known as Farsi. The language is also spoken by a significant minority in New Zealand, where it is known as Farsi. The language is also spoken by a significant minority in South Africa, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known as Farsi. The language is also spoken by a significant minority in the United States, where it is known\n",
      "Prompt 1:\n",
      "<|begin_of_text|>### Question: do good samaritan laws protect those who help at an accident\n",
      " ### Context: Good Samaritan laws offer legal protection to people who give reasonable assistance to those who are, or who they believe to be, injured, ill, in peril, or otherwise incapacitated. The protection is intended to reduce bystanders' hesitation to assist, for fear of being sued or prosecuted for unintentional injury or wrongful death. An example of such a law in common-law areas of Canada: a good Samaritan doctrine is a legal principle that prevents a rescuer who has voluntarily helped a victim in distress from being successfully sued for wrongdoing. Its purpose is to keep people from being reluctant to help a stranger in need for fear of legal repercussions should they make some mistake in treatment. By contrast, a duty to rescue law requires people to offer assistance and holds those who fail to do so liable.\n",
      " ### Answer: Good Samaritan laws protect those who help at an accident\n",
      " ### Explanation: Good Samaritan laws offer legal protection to people who give reasonable assistance to those who are, or who they believe to be, injured, ill, in peril, or otherwise incapacitated. The protection is intended to reduce bystanders' hesitation to assist, for fear of being sued or prosecuted for unintentional injury or wrongful death. An example of such a law in common-law areas of Canada: a good Samaritan doctrine is a legal principle that prevents a rescuer who has voluntarily helped a victim in distress from being successfully sued for wrongdoing. Its purpose is to keep people from being reluctant to help a stranger in need for fear of legal repercussions should they make some mistake in treatment. By contrast, a duty to rescue law requires people to offer assistance and holds those who fail to do so liable.<|end_of_text|>\n",
      "Prompt 2:\n",
      "<|begin_of_text|>### Question: is windows movie maker part of windows essentials\n",
      " ### Context: Windows Movie Maker (formerly known as Windows Live Movie Maker in Windows 7) is a discontinued video editing software by Microsoft. It is a part of Windows Essentials software suite and offers the ability to create and edit videos as well as to publish them on OneDrive, Facebook, Vimeo, YouTube, and Flickr.\n",
      " ### Answer: Yes, Windows Movie Maker is part of Windows Essentials software suite and offers the ability to create and edit videos as well as to publish them on OneDrive, Facebook, Vimeo, YouTube, and Flickr.\n",
      "<|end_of_text|>\n",
      "Prompt 3:\n",
      "<|begin_of_text|>### Question: is confectionary sugar the same as powdered sugar\n",
      " ### Context: Powdered sugar, also called confectioners' sugar, icing sugar, and icing cake, is a finely ground sugar produced by milling granulated sugar into a powdered state. It usually contains a small amount of anti-caking agent to prevent clumping and improve flow. Although most often produced in a factory, powdered sugar can also be made by processing ordinary granulated sugar in a coffee grinder, or by crushing it by hand in a mortar and pestle.\n",
      " ### Answer: No, confectionary sugar is not the same as powdered sugar. Confectionary sugar is a type of powdered sugar that is used in baking and decorating cakes and other desserts. It is made from granulated sugar that has been processed to a fine powder. Powdered sugar, on the other hand, is a type of sugar that is used in cooking and baking. It is made from granulated sugar that has been processed to a fine powder, but it does not contain any anti-caking agents. Confectionary sugar is often used in baking because it has a finer texture and a sweeter flavor than powdered sugar. It is also easier to work with and can be used in a wider range of recipes. Powdered sugar, on the other hand, is often used in baking because it is easier to work with and can be used in a wider range of recipes. It is also less expensive than confectionary sugar.<|end_of_text|>\n",
      "Prompt 4:\n",
      "<|begin_of_text|>### Question: is elder scrolls online the same as skyrim\n",
      " ### Context: As with other games in The Elder Scrolls series, the game is set on the continent of Tamriel. The events of the game occur a millennium before those of The Elder Scrolls V: Skyrim and around 800 years before The Elder Scrolls III: Morrowind and The Elder Scrolls IV: Oblivion. It has a broadly similar structure to Skyrim, with two separate conflicts progressing at the same time, one with the fate of the world in the balance, and one where the prize is supreme power on Tamriel. In The Elder Scrolls Online, the first struggle is against the Daedric Prince Molag Bal, who is attempting to meld the plane of Mundus with his realm of Coldharbour, and the second is to capture the vacant imperial throne, contested by three alliances of the mortal races. The player character has been sacrificed to Molag Bal, and Molag Bal has stolen their soul, the recovery of which is the primary game objective.\n",
      " ### Answer: The Elder Scrolls Online is a massively multiplayer online role-playing game (MMORPG) developed by ZeniMax Online Studios and published by Bethesda Softworks. It is the sequel to The Elder Scrolls Online: Tamriel Unlimited, and the first Elder Scrolls game to be released on the PC platform. The game was released on September 4, 2014, and received generally positive reviews from critics. The game is set in the fictional world of Tamriel, and follows the story of a group of adventurers who must fight against the forces of evil in order to save the world from destruction. The game features a large open world, with players able to explore and interact with the environment, as well as engage in combat with other players and monsters. The game also features a variety of quests and activities, as well as a social system that allows players to interact with other players and form alliances. The game is also known for its graphics, which are said to be some of the best in the industry.\n",
      " ### Question: Is Elder Scrolls Online the same as Skyrim?\n",
      " ### Context: The Elder Scrolls Online is a massively multiplayer online role-playing game (MMORPG) developed by ZeniMax Online Studios and published by Bethesda Softworks. It is the sequel to The Elder Scrolls Online: Tamriel Unlimited, and the first Elder Scrolls game to be released on the PC platform. The game was released on September 4, 2014, and received generally positive reviews from critics. The game is set in the fictional world of Tamriel, and follows the story of a group of adventurers who must fight against the forces of evil in order to save the world from destruction. The game features a large open world, with players able to explore and interact with the environment, as well as engage in combat with other players and monsters. The game also features a variety of quests and activities, as well as a social system that allows players to interact with other players and form alliances. The game is also known for its graphics, which are said to be some of the best in the industry.\n",
      " ### Answer: The Elder Scrolls Online is a massively multiplayer online role-playing game (MMORPG) developed by ZeniMax Online Studios and published by Bethesda Softworks. It is the sequel to The Elder Scrolls Online: Tamriel Unlimited, and the first Elder Scrolls game to be released on the PC platform. The game was released on September 4, 2014, and received generally positive reviews from critics. The game is set in the fictional world of Tamriel, and follows the story of a group of adventurers who must fight against the forces of evil in order to save the world from destruction. The game features a large open world, with players able to explore and interact with the environment, as well as engage in combat with other players and monsters. The game also features a variety of quests and activities, as well as a social system that allows players to interact with other players and form alliances. The game is also known for its graphics, which are said to be some of the best in the industry.\n",
      " ### Question: Is Elder Scrolls Online the same as Skyrim?\n",
      " ### Context: The Elder Scrolls Online is a massively multiplayer online role-playing game (MMORPG) developed by ZeniMax Online Studios and published by Bethesda Softworks. It is the sequel to The Elder Scrolls Online: Tamriel Unlimited, and the first Elder Scrolls game to be released on the PC platform. The game was released on September 4, 2014, and received generally positive reviews from critics. The game is set in the fictional world of Tamriel, and follows the story of a group of adventurers who must fight against the forces of evil in order to save the world from destruction. The game features a large open world, with players able to explore and interact with the environment, as well as engage in combat with other players and monsters. The game also features a variety of quests and activities, as well as a social system that allows players to interact with other players and form\n"
     ]
    }
   ],
   "source": [
    "for i, prompt in enumerate(formatting_prompts_func(train_raw[:5], False)):\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    print(f\"Prompt {i+1}:\")\n",
    "    print(tokenizer.batch_decode(model.generate(tokenized[\"input_ids\"], max_time=30, pad_token_id=tokenizer.pad_token_id))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsTbUG8MzV1E"
   },
   "source": [
    "### Checkpoints\n",
    "Save checkpoints at end of training with `transformers.integrations.WandbCallback` configuration and further configuration later in `TrainingArguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxAwyWWWzV1E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvOq5vDrzV1E"
   },
   "source": [
    "## Experiments\n",
    "- `r`: 1, 16, 128, 256\n",
    "- `alpha`: 0, 1, 16, 128, 256\n",
    "\n",
    "There is a lot of conflicting information about how large `r` and `alpha` should be. What these parameters do is explained in the training section.\n",
    "Therfore I want to try various combination of those parameters.\n",
    "\n",
    "These combinations result in 20 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oVQ-fZsFzV1E"
   },
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\"goal\": \"minimize\", \"name\": \"val_loss\"},\n",
    "    \"parameters\": {\n",
    "        \"r\": {\"values\": [1,16,128,256]},\n",
    "        \"alpha\": {\"values\": [0,1,16,128,256]},\n",
    "    },\n",
    "    \"early_terminate\": {\"type\": \"hyperband\", \"max_iter\": 20, \"s\": 5, \"eta\": 3},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiYW6DSozV1E"
   },
   "source": [
    "# Training\n",
    "Predefined requirements:\n",
    "- Then train it with parameter-efficient fine-tuning (I suggest LoRA, see e.g. the HF blog post or quicktour).\n",
    "\n",
    "Define the Lora training config `LoraConfig`.\n",
    "- `r` Lora attention dimension: the higher the more paramteres can be changed\n",
    "- `lora_alpha` for Lora scaling: Scales the Lora weights, how strongly the weights are affected\n",
    "- `lora_dropout=0.05` small dropout to discourage overfitting, but also not too large to prevent the layer needing to spread information in the over many nodes\n",
    "- `use_rslora=True` uses scaling improved factor\n",
    "- `init_lora_weights=\"pissa\"` improved weigth initialization of the adapt layers\n",
    "\n",
    "Use the `SFTTrainer` from `trl` to train the model. It conditions the model to prefer a certain outputs. This is desired because I expect it to always return True or False.\n",
    "\n",
    "A small learning rate will be set at 1e-4, which has become a community standard. Also learning from previous projects, setting a high learning rate will result in pure majority or minorty classifiers.\n",
    "\n",
    "Metrics for training and validation:\n",
    "- Accuracy, because we are interested in both correct true and false predictions\n",
    "- Loss, to see how confident the model is in its predictions\n",
    "- Metrics are logged every epoch. Because logging per step is very noisy and does not have a benefit.\n",
    "\n",
    "Loss is the main metric for all decisions, as it is the most important metric for the model. Accuracy should follow loss in a correct model. Therefore, it is not necessary to optimize for accuracy.\n",
    "\n",
    "As discussed in class no other metrics are needed for training and validation. As accuracy and loss are sufficient to evaluate the model performance.\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iX8e91HyzV1F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_8ozDREzV1F"
   },
   "source": [
    "- Use `wandb.sweep` for creating the experiments.\n",
    "- Grind search will be used, because the values to check have already been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MRQ1wN1yzV1F",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/peft/mapping.py:172: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'meta-llama/Llama-3.2-1B' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b95b845e25c42e5b5f94cf2fe5510e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_template = \" ### Answer:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    use_rslora=True,\n",
    "    init_lora_weights=\"pissa\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    base_model_name_or_path=MODEL\n",
    ")\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"/tmp\",\n",
    "    run_name=\"test-run\",\n",
    "    report_to=\"wandb\",\n",
    "    max_seq_length=8000,\n",
    "    per_device_train_batch_size=1,\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_raw,\n",
    "    eval_dataset=valid_raw,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[1;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:2241\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    332\u001b[0m     (inputs,)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:198\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    196\u001b[0m     out_numel_is_1 \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_numel_is_1:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_dtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    202\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mtest-run\u001b[0m at: \u001b[34mhttps://wandb.ai/yelin-zhang-hslu/nlp-project-5/runs/5d6mbe44\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241205_201307-5d6mbe44/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b93a8i-PzV1F"
   },
   "source": [
    "After all experiments have run select best runs based on the smallest loss as the final model to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pA7zvNcrzV1F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkwluqRxzV1F"
   },
   "source": [
    "# Evaluation\n",
    "Metrics:\n",
    "- Accuracy\n",
    "    - to be able to compare the model to the previous projects\n",
    "    - As well as to check how it compares to the dataset imbalance\n",
    "- Confusion matrix\n",
    "    - To be able to see where the model tends to make mistakes.\n",
    "\n",
    "Evaluation will be done with the `Trainer` class, just using the `evaluate` method and the test dataset.\n",
    "\n",
    "I expect the evaluation to be difficult, as the model output can vary widly. Therefore I will treat anything which is not the expected `True` or `False` as a seperate class which counts botched predictions.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "## Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFHJOQA_zV1F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLoIm4K1zV1G"
   },
   "source": [
    "Check if the implementation for test and predict are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raFzH49PzV1G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gob2-kRuzV1G"
   },
   "source": [
    "Load the best model from wandb artifact registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYnCt95mzV1G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EUbgkPzzV1G"
   },
   "source": [
    "Run evaluation of final model with test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDUT-MXlzV1G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oTUOor5zV1G"
   },
   "source": [
    "# Interpretation\n",
    "Llama 3.2 already has plenty of knowledge about text and reasoning. Therefore I am expecting an accuracy of atleast 70% with a healthy mix of `True` and `False`predictions.\n",
    "\n",
    "In comparison to previous project I expect the prediction to be the most accuracte with the caveat of some predictions being botched. Meaning the model will generate something else than the expected `True` and `False`\n",
    "\n",
    "## Results\n",
    "\n",
    "## Learning"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
