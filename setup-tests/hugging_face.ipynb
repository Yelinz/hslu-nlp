{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e265922",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "In this notebook, we'll get to know the Hugging Face ecosystem by loading a dataset, encoding the input data, running a model, and evaluating the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261d2d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cbbaad",
   "metadata": {},
   "source": [
    "Take a look at the [Hugging Face datasets hub](https://huggingface.co/datasets). Find the MRPC (Microsoft Research Paraphrase Corpus) dataset that is part of the GLUE (General Language Understanding Evaluation) benchmark. Download the validation split of the dataset with dataset's `load_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673e7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nyu-mll/glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4f191",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "With Transformers (we will get to know them in more detail later in the course), tokenization has become part of the model itself. We first install Hugging Face's transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "772fb9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4933fc",
   "metadata": {},
   "source": [
    "Use the [model page of the base-uncased version of BERT](https://huggingface.co/bert-base-uncased) to initialize a `BertTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd20df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ccab3",
   "metadata": {},
   "source": [
    "Encode the first sentence of the first example in the dataset. Look at the outputs of the following functions:\n",
    "- `tokenizer(sentence)`\n",
    "- `tokenizer.encode(sentence)`\n",
    "- `tokenizer.tokenize(sentence)`\n",
    "- `tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "928c8a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\n",
      "{'input_ids': [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102]\n",
      "['am', '##ro', '##zi', 'accused', 'his', 'brother', ',', 'whom', 'he', 'called', '\"', 'the', 'witness', '\"', ',', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.']\n",
      "[2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012]\n"
     ]
    }
   ],
   "source": [
    "sentence = ds[\"train\"][0][\"sentence1\"]\n",
    "print(sentence)\n",
    "tokens = tokenizer(sentence)\n",
    "encoded = tokenizer.encode(sentence)\n",
    "tokenized = tokenizer.tokenize(sentence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "print(tokens)\n",
    "print(encoded)\n",
    "print(tokenized)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44f21a",
   "metadata": {},
   "source": [
    "**Decoding.** Check out the various ways of decoding: `.decode`, `.convert_ids_to_tokens`, `.convert_tokens_to_string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee2d203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] amrozi accused his brother, whom he called \" the witness \", of deliberately distorting his evidence. [SEP] ['am', '##ro', '##zi', 'accused', 'his', 'brother', ',', 'whom', 'he', 'called', '\"', 'the', 'witness', '\"', ',', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.'] amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.decode(encoded),\n",
    "    tokenizer.convert_ids_to_tokens(ids),\n",
    "    tokenizer.convert_tokens_to_string(tokenized)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748080cf",
   "metadata": {},
   "source": [
    "Use the NLP section of the [quickstart guide](https://huggingface.co/docs/datasets/quickstart) to apply encoding to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db39f62-e353-40f2-b24b-9c08a18f95de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(ds[\"train\"][0][\"sentence1\"], ds[\"train\"][0][\"sentence2\"], truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4c07753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598321310d0a44cf90fd3f8bb7fde353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bdff101fe74d08990c56c7e0afaf41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dacb2cb269c4627a0b7a9d7ae374b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "ds_encoded = ds.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc640318",
   "metadata": {},
   "source": [
    "We have to rename the \"label\" column to \"labels\" to match the expected name in BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "751be0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd73005397646ab92bd50c4f585bfa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc42b10d2cb4b14b69281ba57212cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a4a3c2414a4bc2a6081f4544b319cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_encoded = ds_encoded.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9bd126",
   "metadata": {},
   "source": [
    "- Use the guide again to set the data format to \"torch\". Make sure the columns `input_ids`, `token_type_ids`, `attention_mask` and `labels` are present.\n",
    "- Create a data loader with a `batch_size` of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38eda65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ds_encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "dataloader = torch.utils.data.DataLoader(ds_encoded[\"train\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c6e2bc",
   "metadata": {},
   "source": [
    "## Model\n",
    "We now load a pretrained BERT model and perform sequence classification on the MRPC dataset. Load the `BertForSequenceClassification` model. Set the model to evaluation mode by calling `.eval()` on the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "955542a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead6fcf",
   "metadata": {},
   "source": [
    "In the evaluation state, no gradient information will be saved in the forward pass, and no dropout will be applied (and the values rescaled to match the training's output distribution). We can always set it back to train mode with `.train()`.\n",
    "\n",
    "Additionally, we should call the model in a `torch.no_grad()` context, which sets all the tensors' `.requires_grad` fields to False.\n",
    "\n",
    "## Forward pass\n",
    "Now we run the model on a single batch. Get a batch from the dataloader, pass it to the model's forward function. It is preferred to use `model(.)` to do this instead of `model.forward(.)`. Some hooks may not be run if you use the latter version, as mentioned in this [PyTorch forum question](https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690).\n",
    "\n",
    "- Run a single batch through the model.\n",
    "- Get the output logits\n",
    "- Run a softmax function on it (use `torch.nn.functional.softmax`) to get output probabilities\n",
    "- Display the result (i.e. is sentence2 a paraphrase of sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31c562f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2572,  3217,  ...,     0,     0,     0],\n",
      "        [  101,  9805,  3540,  ...,     0,     0,     0],\n",
      "        [  101,  2027,  2018,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1996,  2922,  ...,     0,     0,     0],\n",
      "        [  101,  6202,  1999,  ...,     0,     0,     0],\n",
      "        [  101, 16565,  2566,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 0])}\n",
      "SequenceClassifierOutput(loss=tensor(1.1341, grad_fn=<NllLossBackward0>), logits=tensor([[-1.4017,  1.6701],\n",
      "        [-0.1502,  0.4794],\n",
      "        [ 2.5819, -2.1943],\n",
      "        [ 0.4769, -0.1553],\n",
      "        [ 2.8182, -2.2834],\n",
      "        [ 1.7621, -1.1680],\n",
      "        [ 2.7745, -2.3235],\n",
      "        [-0.1471,  0.5380],\n",
      "        [ 2.9172, -2.5760],\n",
      "        [ 3.0957, -2.3667],\n",
      "        [-1.8877,  2.0529],\n",
      "        [ 2.1818, -1.7648],\n",
      "        [ 0.0191,  0.3244],\n",
      "        [-2.1468,  2.2737],\n",
      "        [ 0.1368,  0.1174],\n",
      "        [ 0.6216, -0.2077],\n",
      "        [-0.0289,  0.4315],\n",
      "        [-1.5761,  1.7908],\n",
      "        [ 0.1461,  0.2347],\n",
      "        [ 1.0435, -0.5060],\n",
      "        [ 1.4700, -0.9262],\n",
      "        [ 0.6232, -0.1417],\n",
      "        [ 0.7680, -0.3189],\n",
      "        [-0.0509,  0.4191],\n",
      "        [ 1.0695, -0.6420],\n",
      "        [-0.3750,  0.6194],\n",
      "        [ 0.8454, -0.6424],\n",
      "        [-2.2103,  2.3887],\n",
      "        [-1.4238,  1.5506],\n",
      "        [ 0.3032,  0.0222],\n",
      "        [-1.0319,  1.1520],\n",
      "        [ 3.7813, -3.5405]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4327/2976874928.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = torch.nn.functional.softmax(output.logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.4284e-02, 9.5572e-01],\n",
      "        [3.4760e-01, 6.5240e-01],\n",
      "        [9.9164e-01, 8.3575e-03],\n",
      "        [6.5300e-01, 3.4700e-01],\n",
      "        [9.9395e-01, 6.0498e-03],\n",
      "        [9.4932e-01, 5.0685e-02],\n",
      "        [9.9393e-01, 6.0720e-03],\n",
      "        [3.3512e-01, 6.6488e-01],\n",
      "        [9.9590e-01, 4.0979e-03],\n",
      "        [9.9577e-01, 4.2255e-03],\n",
      "        [1.9066e-02, 9.8093e-01],\n",
      "        [9.8104e-01, 1.8956e-02],\n",
      "        [4.2427e-01, 5.7573e-01],\n",
      "        [1.1885e-02, 9.8811e-01],\n",
      "        [5.0484e-01, 4.9516e-01],\n",
      "        [6.9621e-01, 3.0379e-01],\n",
      "        [3.8690e-01, 6.1310e-01],\n",
      "        [3.3346e-02, 9.6665e-01],\n",
      "        [4.7785e-01, 5.2215e-01],\n",
      "        [8.2484e-01, 1.7516e-01],\n",
      "        [9.1653e-01, 8.3466e-02],\n",
      "        [6.8242e-01, 3.1758e-01],\n",
      "        [7.4778e-01, 2.5222e-01],\n",
      "        [3.8462e-01, 6.1538e-01],\n",
      "        [8.4704e-01, 1.5296e-01],\n",
      "        [2.7004e-01, 7.2996e-01],\n",
      "        [8.1575e-01, 1.8425e-01],\n",
      "        [9.9617e-03, 9.9004e-01],\n",
      "        [4.8598e-02, 9.5140e-01],\n",
      "        [5.6978e-01, 4.3022e-01],\n",
      "        [1.0121e-01, 8.9879e-01],\n",
      "        [9.9934e-01, 6.6058e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_batch = next(iter(dataloader))\n",
    "print(train_batch)\n",
    "output = model(**train_batch)\n",
    "print(output)\n",
    "probs = torch.nn.functional.softmax(output.logits)\n",
    "print(probs)\n",
    "#TODO interpret logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaf20648-9b63-4d64-a019-f35cb6327a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(probs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a8dcea",
   "metadata": {},
   "source": [
    "**Question:** Load the model again (execute the cell just below the [Model](#Model) section), run the forward pass and your evaluation. Why do you get different results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227748e1",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f1ff9",
   "metadata": {},
   "source": [
    "## Evaluate a trained model\n",
    "We now download a different model instead: `textattack/bert-base-uncased-QQP`. This is a model trained to detect duplicate questions on Quora, so basically our paraphrase detection task, but trained on a different dataset. Let's see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51a6bf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"textattack/bert-base-uncased-QQP\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-QQP\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ed301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
